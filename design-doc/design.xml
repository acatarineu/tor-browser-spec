<?xml version="1.0" encoding="ISO-8859-1"?>
<!DOCTYPE article PUBLIC "-//OASIS//DTD DocBook XML V4.4//EN"
     "file:///usr/share/sgml/docbook/xml-dtd-4.4-1.0-30.1/docbookx.dtd">

<article id="design">
 <articleinfo>
  <title>The Design and Implementation of the Tor Browser [DRAFT]</title>
   <author>
    <firstname>Mike</firstname><surname>Perry</surname>
    <affiliation>
     <address><email>mikeperry#torproject org</email></address>
    </affiliation>
   </author>
   <author>
    <firstname>Erinn</firstname><surname>Clark</surname>
    <affiliation>
     <address><email>erinn#torproject org</email></address>
    </affiliation>
   </author>
   <author>
    <firstname>Steven</firstname><surname>Murdoch</surname>
    <affiliation>
     <address><email>sjmurdoch#torproject org</email></address>
    </affiliation>
   </author>
   <author>
    <firstname>Georg</firstname><surname>Koppen</surname>
    <affiliation>
     <address><email>gk#torproject org</email></address>
    </affiliation>
   </author>
   <pubdate>January 25th, 2018</pubdate>
 </articleinfo>

<sect1>
  <title>Introduction</title>
  <para>

This document describes the <link linkend="adversary">adversary model</link>,
<link linkend="DesignRequirements">design requirements</link>, and <link
linkend="Implementation">implementation</link> <!-- and <link
linkend="Packaging">packaging</link> and <link linkend="Testing">testing
procedures</link> --> of the Tor Browser. It is current as of Tor Browser
7.0.11.

  </para>
  <para>

This document is also meant to serve as a set of design requirements and to
describe a reference implementation of a Private Browsing Mode that defends
against active network adversaries, in addition to the passive forensic local
adversary currently addressed by the major browsers.

  </para>

  <para>

For more practical information regarding Tor Browser development, please
consult the <ulink
url="https://trac.torproject.org/projects/tor/wiki/doc/TorBrowser/Hacking">Tor
Browser Hacking Guide</ulink>.

  </para>

  <sect2 id="components">
   <title>Browser Component Overview</title>
   <para>

The Tor Browser is based on <ulink
url="https://www.mozilla.org/en-US/firefox/organizations/">Mozilla's Extended
Support Release (ESR) Firefox branch</ulink>. We have a <ulink
url="https://gitweb.torproject.org/tor-browser.git">series of patches</ulink>
against this browser to enhance privacy and security. Browser behavior is
additionally augmented through the <ulink
url="https://gitweb.torproject.org/torbutton.git/tree/">Torbutton
extension</ulink>, though we are in the process of moving this functionality
into direct Firefox patches. We also <ulink
url="https://gitweb.torproject.org/tor-browser.git/tree/browser/app/profile/000-tor-browser.js?h=tor-browser-52.5.2esr-7.0-2">change
a number of Firefox preferences</ulink> from their defaults.

   </para>
   <para>
Tor process management and configuration is accomplished through the <ulink
url="https://gitweb.torproject.org/tor-launcher.git">Tor Launcher</ulink>
addon, which provides the initial Tor configuration splash screen and
bootstrap progress bar. Tor Launcher is also compatible with Thunderbird,
Instantbird, and XULRunner.

   </para>
   <para>

To help protect against potential Tor Exit Node eavesdroppers, we include
<ulink url="https://www.eff.org/https-everywhere">HTTPS-Everywhere</ulink>. To
provide users with optional defense-in-depth against JavaScript and other
potential exploit vectors, we also include <ulink
url="https://noscript.net/">NoScript</ulink>. We also modify <ulink
url="https://gitweb.torproject.org/builders/tor-browser-bundle.git/tree/Bundle-Data/linux/Data/Browser/profile.default/preferences/extension-overrides.js">several
extension preferences</ulink> from their defaults.

   </para>
   <para>

To provide censorship circumvention in areas where the public Tor network is
blocked either by IP, or by protocol fingerprint, we include several <ulink
url="https://trac.torproject.org/projects/tor/wiki/doc/AChildsGardenOfPluggableTransports">Pluggable
Transports</ulink> in the distribution. As of this writing, we include <ulink
url="https://gitweb.torproject.org/pluggable-transports/obfs4.git">Obfs3proxy,
Obfs4proxy</ulink>,
<ulink
url="https://trac.torproject.org/projects/tor/wiki/doc/meek">meek</ulink>,
and <ulink url="https://fteproxy.org/">FTE</ulink>.

   </para>

  </sect2>
</sect1>

<!--
- Design overview and philosophy
  - Security requirements [Torbutton]
    + local leaks?
    - state issues
  - Privacy Requirements [Mostly blog post]
    - Avoid Cross-Domain Linkability
      - Identifiers
      - Fingerprinting
    - 100% self-contained
      - Does not share state with other modes/browsers
      - Easy to remove + wipe with external tools
    - click-to-play for "troublesome" features
   - Philosophy
    - No filters
-->

<sect1 id="DesignRequirements">
  <title>Design Requirements and Philosophy</title>
  <para>

The Tor Browser Design Requirements are meant to describe the properties of a
Private Browsing Mode that defends against both network and local forensic
adversaries.

  </para>
  <para>

There are two main categories of requirements: <link
linkend="security">Security Requirements</link>, and <link
linkend="privacy">Privacy Requirements</link>. Security Requirements are the
minimum properties in order for a browser to be able to support Tor and
similar privacy proxies safely. Privacy requirements are the set of properties
that cause us to prefer one browser over another.

  </para>
  <para>

While we will endorse the use of browsers that meet the security requirements,
it is primarily the privacy requirements that cause us to maintain our own
browser distribution.

  </para>
  <para>

      The key words "MUST", "MUST NOT", "REQUIRED", "SHALL", "SHALL
      NOT", "SHOULD", "SHOULD NOT", "RECOMMENDED",  "MAY", and
      "OPTIONAL" in this document are to be interpreted as described in
      <ulink url="https://www.ietf.org/rfc/rfc2119.txt">RFC 2119</ulink>.

  </para>

  <sect2 id="security">
   <title>Security Requirements</title>
   <para>

The security requirements are primarily concerned with ensuring the safe use
of Tor. Violations in these properties typically result in serious risk for
the user in terms of immediate deanonymization and/or observability. With
respect to browser support, security requirements are the minimum properties
in order for Tor to support the use of a particular browser.

   </para>

<orderedlist>
 <listitem><link linkend="proxy-obedience"><command>Proxy
Obedience</command></link>
 <para>The browser
MUST NOT bypass Tor proxy settings for any content.</para></listitem>

 <listitem><link linkend="state-separation"><command>State
Separation</command></link>

 <para>

The browser MUST NOT provide the content window with any state from any other
browsers or any non-Tor browsing modes. This includes shared state from
independent plugins, and shared state from operating system implementations of
TLS and other support libraries.

</para></listitem>

 <listitem><link linkend="disk-avoidance"><command>Disk
Avoidance</command></link>

<para>

The browser MUST NOT write any information that is derived from or that
reveals browsing activity to the disk, or store it in memory beyond the
duration of one browsing session, unless the user has explicitly opted to
store their browsing history information to disk.

</para></listitem>
 <listitem><link linkend="app-data-isolation"><command>Application Data
Isolation</command></link>

<para>

The components involved in providing private browsing MUST be self-contained,
or MUST provide a mechanism for rapid, complete removal of all evidence of the
use of the mode. In other words, the browser MUST NOT write or cause the
operating system to write <emphasis>any information</emphasis> about the use
of private browsing to disk outside of the application's control. The user
must be able to ensure that secure deletion of the software is sufficient to
remove evidence of the use of the software. All exceptions and shortcomings
due to operating system behavior MUST be wiped by an uninstaller. However, due
to permissions issues with access to swap, implementations MAY choose to leave
it out of scope, and/or leave it to the operating system/platform to implement
ephemeral-keyed encrypted swap.

</para></listitem>

</orderedlist>

  </sect2>

  <sect2 id="privacy">
   <title>Privacy Requirements</title>
   <para>

The privacy requirements are primarily concerned with reducing linkability:
the ability for a user's activity on one site to be linked with their activity
on another site without their knowledge or explicit consent. With respect to
browser support, privacy requirements are the set of properties that cause us
to prefer one browser over another.

   </para>

   <para>

For the purposes of the unlinkability requirements of this section as well as
the descriptions in the <link linkend="Implementation">implementation
section</link>, a <command>URL bar origin</command> means at least the
second-level DNS name.  For example, for mail.google.com, the origin would be
google.com. Implementations MAY, at their option, restrict the URL bar origin
to be the entire fully qualified domain name.

   </para>

<orderedlist>
 <listitem><link linkend="identifier-linkability"><command>Cross-Origin
Identifier Unlinkability</command></link>
  <para>

User activity on one URL bar origin MUST NOT be linkable to their activity in
any other URL bar origin by any third party automatically or without user
interaction or approval. This requirement specifically applies to linkability
from stored browser identifiers, authentication tokens, and shared state. The
requirement does not apply to linkable information the user manually submits
to sites, or due to information submitted during manual link traversal. This
functionality SHOULD NOT interfere with interactive, click-driven federated
login in a substantial way.

  </para>
 </listitem>
 <listitem><link linkend="fingerprinting-linkability"><command>Cross-Origin
Fingerprinting Unlinkability</command></link>
  <para>

User activity on one URL bar origin MUST NOT be linkable to their activity in
any other URL bar origin by any third party. This property specifically applies to
linkability from fingerprinting browser behavior.

  </para>
 </listitem>
 <listitem><link linkend="new-identity"><command>Long-Term
Unlinkability</command></link>
  <para>

The browser MUST provide an obvious, easy way for the user to remove all of
its authentication tokens and browser state and obtain a fresh identity.
Additionally, the browser SHOULD clear linkable state by default automatically
upon browser restart, except at user option.

  </para>
 </listitem>
</orderedlist>

  </sect2>
  <sect2 id="philosophy">
  <title>Philosophy</title>
   <para>

In addition to the above design requirements, the technology decisions about
Tor Browser are also guided by some philosophical positions about technology.

   </para>
   <orderedlist>
     <listitem><command>Preserve existing user model</command>
      <para>

The existing way that the user expects to use a browser must be preserved. If
the user has to maintain a different mental model of how the sites they are
using behave depending on tab, browser state, or anything else that would not
normally be what they experience in their default browser, the user will
inevitably be confused. They will make mistakes and reduce their privacy as a
result. Worse, they may just stop using the browser, assuming it is broken.

      </para>
      <para>

User model breakage was one of the <ulink
url="https://blog.torproject.org/blog/toggle-or-not-toggle-end-torbutton">failures
of Torbutton</ulink>: Even if users managed to install everything properly,
the toggle model was too hard for the average user to understand, especially
in the face of accumulating tabs from multiple states crossed with the current
Tor-state of the browser.

      </para>
     </listitem>
     <listitem><command>Favor the implementation mechanism least likely to
break sites</command>
      <para>

In general, we try to find solutions to privacy issues that will not induce
site breakage, though this is not always possible.

      </para>
     </listitem>
     <listitem><command>Plugins must be restricted</command>
      <para>

Even if plugins always properly used the browser proxy settings (which none of
them do) and could not be induced to bypass them (which all of them can), the
activities of closed-source plugins are very difficult to audit and control.
They can obtain and transmit all manner of system information to websites,
often have their own identifier storage for tracking users, and also
contribute to fingerprinting.

      </para>
      <para>

Therefore, if plugins are to be enabled in private browsing modes, they must
be restricted from running automatically on every page (via click-to-play
placeholders), and/or be sandboxed to restrict the types of system calls they
can execute. If the user agent allows the user to craft an exemption to allow
a plugin to be used automatically, it must only apply to the top level URL bar
domain, and not to all sites, to reduce cross-origin fingerprinting
linkability.

       </para>
     </listitem>
     <listitem><command>Minimize Global Privacy Options</command>
      <para>

<ulink url="https://trac.torproject.org/projects/tor/ticket/3100">Another
failure of Torbutton</ulink> was the options panel. Each option
that detectably alters browser behavior can be used as a fingerprinting tool.
Similarly, all extensions <ulink
url="https://blog.chromium.org/2010/06/extensions-in-incognito.html">should be
disabled in the mode</ulink> except as an opt-in basis. We should not load
system-wide and/or operating system provided addons or plugins.

     </para>
     <para>
Instead of global browser privacy options, privacy decisions should be made
<ulink
url="https://wiki.mozilla.org/Privacy/Features/Site-based_data_management_UI">per
URL bar origin</ulink> to eliminate the possibility of linkability
between domains. For example, when a plugin object (or a JavaScript access of
window.plugins) is present in a page, the user should be given the choice of
allowing that plugin object for that URL bar origin only. The same
goes for exemptions to third party cookie policy, geolocation, and any other
privacy permissions.
     </para>
     <para>
If the user has indicated they wish to record local history storage, these
permissions can be written to disk. Otherwise, they should remain memory-only.
     </para>
     </listitem>
     <listitem><command>No filters</command>
      <para>

Site-specific or filter-based addons such as <ulink
url="https://addons.mozilla.org/en-US/firefox/addon/adblock-plus/">AdBlock
Plus</ulink>, <ulink url="https://requestpolicy.com/">Request Policy</ulink>,
<ulink url="https://www.ghostery.com/about-ghostery/">Ghostery</ulink>, <ulink
url="http://priv3.icsi.berkeley.edu/">Priv3</ulink>, and <ulink
url="https://sharemenot.cs.washington.edu/">Sharemenot</ulink> are to be
avoided. We believe that these addons do not add any real privacy to a proper
<link linkend="Implementation">implementation</link> of the above <link
linkend="privacy">privacy requirements</link>, and that development efforts
should be focused on general solutions that prevent tracking by all third
parties, rather than a list of specific URLs or hosts.
     </para>
     <para>
Implementing filter-based blocking directly into the browser, such as done with
<ulink
url="https://ieee-security.org/TC/SPW2015/W2SP/papers/W2SP_2015_submission_32.pdf">
Firefox' Tracking Protection</ulink>, does not alleviate the concerns mentioned
in the previous paragraph. There is still just a list containing specific
URLs and hosts which, in this case, are
<ulink url="https://services.disconnect.me/disconnect-plaintext.json">
assembled</ulink> by <ulink url="https://disconnect.me/trackerprotection">
Disconnect</ulink> and <ulink url="https://github.com/mozilla-services/shavar-list-exceptions">adapted</ulink> by Mozilla.
     </para>
     <para>
Trying to resort to <ulink
url="https://jonathanmayer.org/papers_data/bau13.pdf">filter methods based on
machine learning</ulink> does not solve the problem either: they don't provide
a general solution to the tracking problem as they are working probabilistically.
Even with a precision rate at 99% and a false positive rate at 0.1% trackers
would be missed and sites would be wrongly blocked.
     </para>
     <para>
Filter-based solutions in general can also introduce strange breakage and cause
usability nightmares. For instance, there is a trend to observe that websites
start <ulink url="https://petsymposium.org/2017/papers/issue3/paper25-2017-3-source.pdf">
detecting filer extensions and block access to content</ulink> on them. Coping
with this fallout easily leads to just <ulink
url="https://github.com/mozilla-services/shavar-list-exceptions">whitelisting
</ulink>
the affected domains, hoping that this helps, defeating the purpose of the
filter in the first place. Filters will also fail to do their job if an
adversary simply registers a new domain or <ulink
url="https://ieee-security.org/TC/SPW2015/W2SP/papers/W2SP_2015_submission_24.pdf">
creates a new URL path</ulink>. Worse still, the unique filter sets that each
user creates or installs will provide a wealth of fingerprinting targets.
      </para>
      <para>

As a general matter, we are also generally opposed to shipping an always-on Ad
blocker with Tor Browser. We feel that this would damage our credibility in
terms of demonstrating that we are providing privacy through a sound design
alone, as well as damage the acceptance of Tor users by sites that support
themselves through advertising revenue.

      </para>
      <para>
Users are free to install these addons if they wish, but doing
so is not recommended, as it will alter the browser request fingerprint.
      </para>
     </listitem>
     <listitem><command>Stay Current</command>
      <para>
We believe that if we do not stay current with the support of new web
technologies, we cannot hope to substantially influence or be involved in
their proper deployment or privacy realization. However, we will likely disable
high-risk features pending analysis, audit, and mitigation.
      </para>
     </listitem>
<!--
     <listitem><command>Transparency in Navigation Tracking</command>
      <para>

While we believe it is possible to restrict third party tracking with only
minimal site breakage, it is our long-term goal to further reduce cross-origin
click navigation tracking to mechanisms that are detectable by experts and
attentive users, so they can alert the general public if cross-origin
click navigation tracking is happening where it should not be.

      </para>
      <para>

However, the entrenched nature of certain archaic web features make it
impossible for us to achieve this wider goal by ourselves without substantial
site breakage. So, instead we maintain a <link linkend="deprecate">Deprecation
Wishlist</link> of archaic web technologies that are currently being (ab)used
to facilitate federated login and other legitimate click-driven cross-domain
activity but that can one day be replaced with more privacy friendly,
auditable alternatives.

      </para>
     </listitem>
-->
   </orderedlist>
  </sect2>
</sect1>

<!--
- Implementation
  - Section Template
    - Sub Section
      - "Design Goal":
      - "Implementation Status"
  - Local Privacy
  - Linkability
    - Stored State
      - Cookies
      - Cache
      - DOM Storage
      - HTTP Auth
      - SSL state
    - Plugins
    - Fingerprinting
      - Location + timezone is part of this
  - Patches?
-->
  <sect1 id="adversary">
   <title>Adversary Model</title>
   <para>

A Tor web browser adversary has a number of goals, capabilities, and attack
types that can be used to illustrate the design requirements for the
Tor Browser. Let's start with the goals.

   </para>
   <sect2 id="adversary-goals">
    <title>Adversary Goals</title>
    <orderedlist>
<!-- These aren't really commands.. But it's the closest I could find in an
acceptable style.. Don't really want to make my own stylesheet -->
     <listitem><command>Bypassing proxy settings</command>
     <para>The adversary's primary goal is direct compromise and bypass of
Tor, causing the user to directly connect to an IP of the adversary's
choosing.</para>
     </listitem>
     <listitem><command>Correlation of Tor vs Non-Tor Activity</command>
     <para>If direct proxy bypass is not possible, the adversary will likely
happily settle for the ability to correlate something a user did via Tor with
their non-Tor activity. This can be done with cookies, cache identifiers,
JavaScript events, and even CSS. Sometimes the fact that a user uses Tor may
be enough for some authorities.</para>
     </listitem>
     <listitem><command>History disclosure</command>
     <para>
The adversary may also be interested in history disclosure: the ability to
query a user's history to see if they have issued certain censored search
queries, or visited censored sites.
     </para>
     </listitem>
     <listitem><command>Correlate activity across multiple sites</command>
     <para>

The primary goal of the advertising networks is to know that the user who
visited siteX.com is the same user that visited siteY.com to serve them
targeted ads. The advertising networks become our adversary insofar as they
attempt to perform this correlation without the user's explicit consent.

     </para>
     </listitem>
     <listitem><command>Fingerprinting/anonymity set reduction</command>
     <para>

Fingerprinting (more generally: "anonymity set reduction") is used to attempt
to gather identifying information on a particular individual without the use
of tracking identifiers. If the dissident's or whistleblower's timezone is
available, and they are using a rare build of Firefox for an obscure operating
system, and they have a specific display resolution only used on one type of
laptop, this can be very useful information for tracking them down, or at
least <link linkend="fingerprinting">tracking their activities</link>.

     </para>
     </listitem>
     <listitem><command>History records and other on-disk
information</command>
     <para>

In some cases, the adversary may opt for a heavy-handed approach, such as
seizing the computers of all Tor users in an area (especially after narrowing
the field by the above two pieces of information). History records and cache
data are the primary goals here. Secondary goals may include confirming
on-disk identifiers (such as hostname and disk-logged spoofed MAC address
history) obtained by other means.

     </para>
     </listitem>
    </orderedlist>
   </sect2>

   <sect2 id="adversary-positioning">
    <title>Adversary Capabilities - Positioning</title>
    <para>
The adversary can position themselves at a number of different locations in
order to execute their attacks.
    </para>
    <orderedlist>
     <listitem><command>Exit Node or Upstream Router</command>
     <para>
The adversary can run exit nodes, or alternatively, they may control routers
upstream of exit nodes. Both of these scenarios have been observed in the
wild.
     </para>
     </listitem>
     <listitem><command>Ad servers and/or Malicious Websites</command>
     <para>
The adversary can also run websites, or more likely, they can contract out
ad space from a number of different ad servers and inject content that way. For
some users, the adversary may be the ad servers themselves. It is not
inconceivable that ad servers may try to subvert or reduce a user's anonymity
through Tor for marketing purposes.
     </para>
     </listitem>
     <listitem><command>Local Network/ISP/Upstream Router</command>
     <para>
The adversary can also inject malicious content at the user's upstream router
when they have Tor disabled, in an attempt to correlate their Tor and Non-Tor
activity.
     </para>
     <para>

Additionally, at this position the adversary can block Tor, or attempt to
recognize the traffic patterns of specific web pages at the entrance to the Tor
network.

     </para>
     </listitem>
     <listitem><command>Physical Access</command>
     <para>
Some users face adversaries with intermittent or constant physical access.
Users in Internet cafes, for example, face such a threat. In addition, in
countries where simply using tools like Tor is illegal, users may face
confiscation of their computer equipment for excessive Tor usage or just
general suspicion.
     </para>
     </listitem>
    </orderedlist>
   </sect2>

   <sect2 id="attacks">
    <title>Adversary Capabilities - Attacks</title>
    <para>

The adversary can perform the following attacks from a number of different
positions to accomplish various aspects of their goals. It should be noted
that many of these attacks (especially those involving IP address leakage) are
often performed by accident by websites that simply have JavaScript, dynamic
CSS elements, and plugins. Others are performed by ad servers seeking to
correlate users' activity across different IP addresses, and still others are
performed by malicious agents on the Tor network and at national firewalls.

    </para>
    <orderedlist>
     <listitem><command>Read and insert identifiers</command>
     <para>

The browser contains multiple facilities for storing identifiers that the
adversary creates for the purposes of tracking users. These identifiers are
most obviously cookies, but also include HTTP auth, DOM storage, cached
scripts and other elements with embedded identifiers, client certificates, and
even TLS Session IDs.

     </para>
     <para>

An adversary in a position to perform MITM content alteration can inject
document content elements to both read and inject cookies for arbitrary
domains. In fact, even many "SSL secured" websites are vulnerable to this sort of
<ulink url="http://seclists.org/bugtraq/2007/Aug/0070.html">active
sidejacking</ulink>. In addition, the ad networks of course perform tracking
with cookies as well.

     </para>
     <para>

These types of attacks are attempts at subverting our <link
linkend="identifier-linkability">Cross-Origin Identifier Unlinkability</link> and <link
linkend="new-identity">Long-Term Unlinkability</link> design requirements.

     </para>
     </listitem>
     <listitem id="fingerprinting"><command>Fingerprint users based on browser
attributes</command>
<para>

There is an absurd amount of information available to websites via attributes
of the browser. This information can be used to reduce the anonymity set, or
even uniquely fingerprint individual users. Attacks of this nature are
typically aimed at tracking users across sites without their consent, in an
attempt to subvert our <link linkend="fingerprinting-linkability">Cross-Origin
Fingerprinting Unlinkability</link> and <link
linkend="new-identity">Long-Term Unlinkability</link> design requirements.

</para>

<para>

Fingerprinting is an intimidating problem to attempt to tackle, especially
without a metric to determine or at least intuitively understand and estimate
which features will most contribute to linkability between visits.

</para>

<para>

The <ulink url="https://panopticlick.eff.org/about">Panopticlick study
done</ulink> by the EFF uses the <ulink
url="https://en.wikipedia.org/wiki/Entropy_%28information_theory%29">Shannon
entropy</ulink> - the number of identifying bits of information encoded in
browser properties - as this metric. Their <ulink
url="https://wiki.mozilla.org/Fingerprinting#Data">result data</ulink> is
definitely useful, and the metric is probably the appropriate one for
determining how identifying a particular browser property is. However, some
quirks of their study means that they do not extract as much information as
they could from display information: they only use desktop resolution and do
not attempt to infer the size of toolbars. In the other direction, they may be
over-counting in some areas, as they did not compute joint entropy over
multiple attributes that may exhibit a high degree of correlation. Also, new
browser features are added regularly, so the data should not be taken as
final.

      </para>
     <para>

Despite the uncertainty, all fingerprinting attacks leverage the following
attack vectors:

     </para>
     <orderedlist>
     <listitem><command>Observing Request Behavior</command>
     <para>

Properties of the user's request behavior comprise the bulk of low-hanging
fingerprinting targets. These include: User agent, Accept-* headers, pipeline
usage, and request ordering. Additionally, the use of custom filters such as
AdBlock and other privacy filters can be used to fingerprint request patterns
(as an extreme example).

     </para>
     </listitem>

     <listitem><command>Inserting JavaScript</command>
     <para>

JavaScript can reveal a lot of fingerprinting information. It provides DOM
objects such as window.screen and window.navigator to extract information
about the user agent.

Also, JavaScript can be used to query the user's timezone via the
<function>Date()</function> object, <ulink
url="https://www.khronos.org/registry/webgl/specs/1.0/#5.13">WebGL</ulink> can
reveal information about the video card in use, and high precision timing
information can be used to <ulink
url="https://cseweb.ucsd.edu/~hovav/dist/jspriv.pdf">fingerprint the cpu and
interpreter speed</ulink>. JavaScript features such as
<ulink url="https://www.w3.org/TR/resource-timing/">Resource Timing</ulink>
may leak an unknown amount of network timing related information. And, moreover,
JavaScript is able to
<ulink url="https://seclab.cs.ucsb.edu/media/uploads/papers/sp2013_cookieless.pdf">
extract</ulink>
<ulink url="https://www.cosic.esat.kuleuven.be/fpdetective/">available</ulink>
<ulink url="https://hal.inria.fr/hal-01285470v2/document">fonts</ulink> on a
device with high precision.

     </para>
     </listitem>

     <listitem><command>Inserting Plugins</command>
     <para>

The Panopticlick project found that the mere list of installed plugins (in
navigator.plugins) was sufficient to provide a large degree of
fingerprintability. Additionally, plugins are capable of extracting font lists,
interface addresses, and other machine information that is beyond what the
browser would normally provide to content. In addition, plugins can be used to
store unique identifiers that are more difficult to clear than standard
cookies.  <ulink url="https://epic.org/privacy/cookies/flash.html">Flash-based
cookies</ulink> fall into this category, but there are likely numerous other
examples. Beyond fingerprinting, plugins are also abysmal at obeying the proxy
settings of the browser.


     </para>
     </listitem>
     <listitem><command>Inserting CSS</command>
     <para>

<ulink url="https://developer.mozilla.org/En/CSS/Media_queries">CSS media
queries</ulink> can be inserted to gather information about the desktop size,
widget size, display type, DPI, user agent type, and other information that
was formerly available only to JavaScript.

     </para>
     </listitem>
     </orderedlist>
     </listitem>
     <listitem id="website-traffic-fingerprinting"><command>Website traffic fingerprinting</command>
     <para>

Website traffic fingerprinting is an attempt by the adversary to recognize the
encrypted traffic patterns of specific websites. In the case of Tor, this
attack would take place between the user and the Guard node, or at the Guard
node itself.
     </para>

     <para> The most comprehensive study of the statistical properties of this
attack against Tor was done by <ulink
url="https://lorre.uni.lu/~andriy/papers/acmccs-wpes11-fingerprinting.pdf">Panchenko
et al</ulink>. Unfortunately, the publication bias in academia has encouraged
the production of
<ulink url="https://blog.torproject.org/blog/critique-website-traffic-fingerprinting-attacks">a
number of follow-on attack papers claiming "improved" success rates</ulink>, in
some cases even claiming to completely invalidate any attempt at defense. These
"improvements" are actually enabled primarily by taking a number of shortcuts
(such as classifying only very small numbers of web pages, neglecting to publish
ROC curves or at least false positive rates, and/or omitting the effects of
dataset size on their results). Despite these subsequent "improvements", we are
skeptical of the efficacy of this attack in a real world scenario,
<emphasis>especially</emphasis> in the face of any defenses.

     </para>
     <para>

In general, with machine learning, as you increase the <ulink
url="https://en.wikipedia.org/wiki/VC_dimension">number and/or complexity of
categories to classify</ulink> while maintaining a limit on reliable feature
information you can extract, you eventually run out of descriptive feature
information, and either true positive accuracy goes down or the false positive
rate goes up. This error is called the <ulink
url="https://www.cs.washington.edu/education/courses/csep573/98sp/lectures/lecture8/sld050.htm">bias
in your hypothesis space</ulink>. In fact, even for unbiased hypothesis
spaces, the number of training examples required to achieve a reasonable error
bound is <ulink
url="https://en.wikipedia.org/wiki/Probably_approximately_correct_learning#Equivalence">a
function of the complexity of the categories</ulink> you need to classify.

     </para>
      <para>


In the case of this attack, the key factors that increase the classification
complexity (and thus hinder a real world adversary who attempts this attack)
are large numbers of dynamically generated pages, partially cached content,
and also the non-web activity of the entire Tor network. This yields an
effective number of "web pages" many orders of magnitude larger than even <ulink
url="https://lorre.uni.lu/~andriy/papers/acmccs-wpes11-fingerprinting.pdf">Panchenko's
"Open World" scenario</ulink>, which suffered continuous near-constant decline
in the true positive rate as the "Open World" size grew (see figure 4). This
large level of classification complexity is further confounded by a noisy and
low resolution featureset - one which is also relatively easy for the defender
to manipulate at low cost.

     </para>
     <para>

To make matters worse for a real-world adversary, the ocean of Tor Internet
activity (at least, when compared to a lab setting) makes it a certainty that
an adversary attempting examine large amounts of Tor traffic will ultimately
be overwhelmed by false positives (even after making heavy tradeoffs on the
ROC curve to minimize false positives to below 0.01%). This problem is known
in the IDS literature as the <ulink
url="http://www.raid-symposium.org/raid99/PAPERS/Axelsson.pdf">Base Rate
Fallacy</ulink>, and it is the primary reason that anomaly and activity
classification-based IDS and antivirus systems have failed to materialize in
the marketplace (despite early success in academic literature).

     </para>
     <para>

Still, we do not believe that these issues are enough to dismiss the attack
outright. But we do believe these factors make it both worthwhile and
effective to <link linkend="traffic-fingerprinting-defenses">deploy
light-weight defenses</link> that reduce the accuracy of this attack by
further contributing noise to hinder successful feature extraction.

     </para>
     </listitem>
     <listitem><command>Remotely or locally exploit browser and/or
OS</command>
     <para>

Last, but definitely not least, the adversary can exploit either general
browser vulnerabilities, plugin vulnerabilities, or OS vulnerabilities to
install malware and surveillance software. An adversary with physical access
can perform similar actions.

    </para>
    <para>

For the purposes of the browser itself, we limit the scope of this adversary
to one that has passive forensic access to the disk after browsing activity
has taken place. This adversary motivates our
<link linkend="disk-avoidance">Disk Avoidance</link> defenses.

    </para>
    <para>

An adversary with arbitrary code execution typically has more power, though.
It can be quite hard to really significantly limit the capabilities of such an
adversary. <ulink
url="https://tails.boum.org/contribute/design/">The Tails system</ulink> can
provide some defense against this adversary through the use of readonly media
and frequent reboots, but even this can be circumvented on machines without
Secure Boot through the use of BIOS rootkits.

     </para>
     </listitem>
    </orderedlist>
   </sect2>

</sect1>

<sect1 id="Implementation">
  <title>Implementation</title>
  <para>

The Implementation section is divided into subsections, each of which
corresponds to a <link linkend="DesignRequirements">Design Requirement</link>.
Each subsection is divided into specific web technologies or properties. The
implementation is then described for that property.

  </para>
  <para>

In some cases, the implementation meets the design requirements in a non-ideal
way (for example, by disabling features). In rare cases, there may be no
implementation at all. Both of these cases are denoted by differentiating
between the <command>Design Goal</command> and the <command>Implementation
Status</command> for each property. Corresponding bugs in the <ulink
url="https://trac.torproject.org/projects/tor/report">Tor bug tracker</ulink>
are typically linked for these cases.

  </para>
  <sect2 id="proxy-obedience">
   <title>Proxy Obedience</title>
   <para>

Proxy obedience is assured through the following:
   </para>
<orderedlist>
 <listitem><command>Firefox proxy settings, patches, and build flags</command>
 <para>

Our <ulink
url="https://gitweb.torproject.org/tor-browser.git/tree/browser/app/profile/000-tor-browser.js?h=tor-browser-52.5.2esr-7.0-2">Firefox
preferences file</ulink> sets the Firefox proxy settings to use Tor directly
as a SOCKS proxy. It sets <command>network.proxy.socks_remote_dns</command>,
<command>network.proxy.socks_version</command>,
<command>network.proxy.socks_port</command>, and
<command>network.dns.disablePrefetch</command>.

 </para>
 <para>

To prevent proxy bypass by WebRTC calls, we disable WebRTC at compile time
with the <command>--disable-webrtc</command> configure switch, as well
as set the pref <command>media.peerconnection.enabled</command> to false.

 </para>
 <para>

We also patch Firefox in order to provide several defense-in-depth mechanisms
for proxy safety. Notably, we <ulink
url="https://gitweb.torproject.org/tor-browser.git/commit/?h=tor-browser-52.5.2esr-7.0-2&amp;id=35ce9974e034c0374fb3c8e00e9eb0231c4f3378">patch
the DNS service</ulink> to prevent any browser or addon DNS resolution, and we
also <ulink url="https://gitweb.torproject.org/tor-browser.git/commit/?h=tor-browser-52.5.2esr-7.0-2&amp;id=ee28d8f27fdb1e47481987535c7da70095042ee2">
remove the DNS lookup for the profile lock signature</ulink>. Furhermore, we
<ulink
url="https://gitweb.torproject.org/tor-browser.git/commit/?h=tor-browser-52.5.2esr-7.0-2&amp;id=ffba8d1b84431b4024d5012b326cbcb986047f27">patch
OCSP and PKIX code</ulink> to prevent any use of the non-proxied command-line
tool utility functions from being functional while linked in to the browser.
In both cases, we could find no direct paths to these routines in the browser,
but it seemed better safe than sorry.

 </para>

 <para>

For further defense-in-depth we disable WebIDE because it can bypass proxy
settings for remote debugging, and also because it downloads extensions we
have not reviewed. We
are doing this by setting
<command>devtools.webide.autoinstallADBHelper</command>,
<command>devtools.webide.autoinstallFxdtAdapters</command>,
<command>devtools.webide.enabled</command>, and
<command>devtools.appmanager.enabled</command> to <command>false</command>.
Moreover, we removed the Roku Screen Sharing and screencaster code with a
<ulink url="https://gitweb.torproject.org/tor-browser.git/commit/?h=tor-browser-52.5.2esr-7.0-2&amp;id=055bdffbef68bc8d5e8005b3c7dd2f5d99da1163">
Firefox patch</ulink> as these features can bypass proxy settings as well.
 </para>

 <para>
Further down on our road to proxy safety we <ulink url="https://gitweb.torproject.org/tor-browser.git/commit/?h=tor-browser-52.5.2esr-7.0-2&amp;id=7222d02638689a64d7297b8e5c202f9c37547523">
disable the network tickler</ulink> as it has the capability to send UDP
traffic and we <ulink url="https://gitweb.torproject.org/tor-browser.git/commit/?h=tor-browser-52.5.2esr-7.0-2&amp;id=5bc957b4f635a659f9aecaa374972ecca7f770a8">
disable mDNS support</ulink>, since mDNS uses UDP packets as well. We also disable
Mozilla's TCPSocket by setting
<command>dom.mozTCPSocket.enabled</command> to <command>false</command>. We
<ulink url="https://trac.torproject.org/projects/tor/ticket/18866">intend to
rip out</ulink> the TCPSocket code in the future to have an even more solid
guarantee that it won't be used by accident.
 </para>

 <para>
Finally, we <ulink url="https://gitweb.torproject.org/tor-browser.git/commit/?h=tor-browser-52.5.2esr-7.0-2&amp;id=55bd129f081bd37ae9e72ae32434fbb56ff4e446">
remove</ulink> potentially unsafe Rust code.
 </para>

 <para>
During every Extended Support Release transition, we perform <ulink
url="https://gitweb.torproject.org/tor-browser-spec.git/tree/audits">in-depth
code audits</ulink> to verify that there were no system calls or XPCOM
activity in the source tree that did not use the browser proxy settings.
 </para>
 <para>

We have verified that these settings and patches properly proxy HTTPS, OCSP,
HTTP, FTP, gopher (now defunct), DNS, SafeBrowsing Queries, all JavaScript
activity, including HTML5 audio and video objects, addon updates, WiFi
geolocation queries, searchbox queries, XPCOM addon HTTPS/HTTP activity,
WebSockets, and live bookmark updates. We have also verified that external
protocol helpers, such as SMB URLs and other custom protocol handlers are all
blocked.
 </para>
</listitem>

 <listitem><command>Disabling plugins</command>

<para>
Plugins, like Flash, have the ability to make arbitrary OS system calls and
<ulink url="https://ip-check.info/">bypass proxy settings</ulink>. This includes
the ability to make UDP sockets and send arbitrary data independent of the
browser proxy settings.
 </para>
 <para>
Torbutton disables plugins by using the
<command>@mozilla.org/plugin/host;1</command> service to mark the plugin tags
as disabled. This block can be undone through both the Torbutton Security UI,
and the Firefox Plugin Preferences.
 </para>
 <para>
If the user does enable plugins in this way, plugin-handled objects are still
restricted from automatic load through Firefox's click-to-play preference
<command>plugins.click_to_play</command>.
 </para>

 <para>

In addition, to reduce any unproxied activity by arbitrary plugins at load
time, and to reduce the fingerprintability of the installed plugin list, we
also patch the Firefox source code to <ulink
url="https://gitweb.torproject.org/tor-browser.git/commit/?h=tor-browser-52.5.2esr-7.0-2&amp;id=95a0100fd8ac0fdbe9f517e9b7ea86d6b77ec2c9">
prevent the load of any plugins except for Flash and Gnash</ulink>. Even for
Flash and Gnash, we also patch Firefox to <ulink
url="https://gitweb.torproject.org/tor-browser.git/commit/?h=tor-browser-52.5.2esr-7.0-2&amp;id=39f5a767c0c082b1e4a001cf685a6efb31bd62c6">
prevent loading them into the address space</ulink> until they are explicitly
enabled.
 </para>
 <para>
With <ulink url="https://wiki.mozilla.org/GeckoMediaPlugins">Gecko Media
Plugins</ulink> (GMPs) a second type of plugins is available. They are mainly
third party codecs and <ulink url="https://www.w3.org/TR/encrypted-media/">EME</ulink>
content decryption modules. We currently disable these plugins as they either
can't be built reproducibly or are binary blobs which we are not allowed to
audit (or both). For the EME case we use the <command>--disable-eme</command>
configure switch and set
<command>browser.eme.ui.enabled</command>,
<command>media.gmp-eme-adobe.visible</command>,
<command>media.gmp-eme-adobe.enabled</command>,
<command>media.gmp-widevinecdm.visible</command>,
<command>media.gmp-widevinecdm.enabled</command>,
<command>media.eme.enabled</command>, and
<command>media.eme.apiVisible</command> to <command>false</command> to indicate
to the user that this feature is disabled. For GMPs in general we make sure that
the external server is not even pinged for updates/downloads in the first place
by setting <command>media.gmp-manager.url.override</command> to
<command>data:text/plain,</command> and avoid any UI with <command>
  media.gmp-provider.enabled</command> set to <command>false</command>. Moreover,
we disable GMP downloads via local fallback by setting
<command>media.gmp-manager.updateEnabled</command> to <command>false</command>.
To reduce our attack surface we exclude the ClearKey EME system, too.

 </para>
 </listitem>
 <listitem><command>External App Blocking and Drag Event Filtering</command>
  <para>

External apps can be induced to load files that perform network activity.
Unfortunately, there are cases where such apps can be launched automatically
with little to no user input. In order to prevent this, we ship <ulink
url="https://gitweb.torproject.org/tor-browser.git/commit/?h=tor-browser-52.5.2esr-7.0-2&amp;id=d179d8a4861199e203934ecc36dd6d8ade549dfa">
Firefox</ulink> <ulink url="https://gitweb.torproject.org/tor-browser.git/commit/?h=tor-browser-52.5.2esr-7.0-2&amp;id=99173c3a5f83d9ac44091a72c5570efd296dff8f">patches</ulink> and Torbutton installs a component to <ulink
url="https://gitweb.torproject.org/torbutton.git/tree/src/components/external-app-blocker.js">
provide the user with a popup</ulink> whenever the browser attempts to launch
a helper application.

  </para>
  <para>

Furthermore, we ship a <ulink url="https://gitweb.torproject.org/tor-browser.git/commit/?h=tor-browser-52.5.2esr-7.0-2&amp;id=d75b79f6fa920e6a1e3043005dfd50060ea70e57">patch for Linux users</ulink> that makes
sure sftp:// and smb:// URLs are not passed along to the operating system as this
can lead to proxy bypasses on systems that have GIO/GnomeVS support. And proxy
bypass risks due to file:// URLs should be mitigated for macOS and Linux users
by <ulink url="https://gitweb.torproject.org/tor-browser.git/commit/?h=tor-browser-52.5.2esr-7.0-2&amp;id=8db44df10d1d82850e8b4cfe81ac3b5fce32a663">
two</ulink> <ulink url="https://gitweb.torproject.org/tor-browser.git/commit/?h=tor-browser-52.5.2esr-7.0-2&amp;id=a8e1fcc8678aa1583f73ef231c99f77cf17196d9">
further patches</ulink>.

  </para>
  <para>

Additionally, modern desktops now pre-emptively fetch any URLs in Drag and
Drop events as soon as the drag is initiated. This download happens
independent of the browser's Tor settings, and can be triggered by something
as simple as holding the mouse button down for slightly too long while
clicking on an image link. We filter drag and drop events events <ulink
url="https://gitweb.torproject.org/torbutton.git/tree/src/components/external-app-blocker.js">from
Torbutton</ulink> before the OS downloads the URLs the events contained.

  </para>
 </listitem>
 <listitem><command>Disabling system extensions and clearing the addon whitelist</command>
  <para>

Firefox addons can perform arbitrary activity on your computer, including
bypassing Tor. It is for this reason we disable the addon whitelist
(<command>xpinstall.whitelist.add</command>), so that users are prompted
before installing addons regardless of the source. We also exclude
system-level addons from the browser through the use of
<command>extensions.enabledScopes</command> and
<command>extensions.autoDisableScopes</command>. Furthermore, we set
<command>extensions.systemAddon.update.url</command> and <command>
extensions.hotfix.id</command> to an empty string in order
to avoid the risk of getting extensions installed by Mozilla into Tor Browser,
and remove unused system extensions with a
<ulink url="https://gitweb.torproject.org/tor-browser.git/commit/?h=tor-browser-52.5.2esr-7.0-2&amp;id=4d90fcf15e328ca369751011ad0a9c0c1ba2f153">
Firefox patch</ulink>.
In order to make it harder for users to accidentally install extensions which
Mozilla presents to them on the <emphasis>about:addons</emphasis> page, we hide
the <emphasis>Get Addons</emphasis> option on it by setting
<command>extensions.getAddons.showPane</command> to <command>false</command>.

  </para>
 </listitem>
 </orderedlist>
  </sect2>
  <sect2 id="state-separation">
   <title>State Separation</title>
   <para>

Tor Browser State is separated from existing browser state through use of a
custom Firefox profile, and by setting the $HOME environment variable to the
root of the bundle's directory. The browser also does not load any
system-wide extensions (through the use of
<command>extensions.enabledScopes</command> and
<command>extensions.autoDisableScopes</command>). Furthermore, plugins are
disabled, which prevents Flash cookies from leaking from a pre-existing Flash
directory.

   </para>
  </sect2>
  <sect2 id="disk-avoidance">
   <title>Disk Avoidance</title>
   <sect3>
    <title>Design Goal:</title>
    <blockquote>

The User Agent MUST (at user option) prevent all disk records of browser activity.
The user SHOULD be able to optionally enable URL history and other history
features if they so desire.

    </blockquote>
   </sect3>
   <sect3>
    <title>Implementation Status:</title>
   <blockquote>
     We are working towards this goal through several mechanisms. First, we set
     the Firefox Private Browsing preference
     <command>browser.privatebrowsing.autostart</command> to <command>true</command>.
     We also had to disable the media cache with the pref <command>media.cache_size</command>, to prevent HTML5 videos from being written to the OS temporary directory, which happened regardless of the private browsing mode setting.
     Finally, we set <command>security.nocertdb</command> to <command>true</command>
     to make the intermediate certificate store memory-only.
   </blockquote>
   <blockquote>
     Moreover, we prevent text leaking from the web console to the /tmp
     directory with a direct <ulink url="https://gitweb.torproject.org/tor-browser.git/commit/?h=tor-browser-52.5.2esr-7.0-2&amp;id=48b68533d113c5998d19d4e5acfb8967ba2d5f5b">Firefox patch</ulink>.
   </blockquote>
   <blockquote>

As an additional defense-in-depth measure, we set
<command>browser.cache.disk.enable</command>,
<command>browser.cache.offline.enable</command>,
<command>signon.rememberSignons</command>,
<command>browser.formfill.enable</command> to <command>true</command>,
<command>browser.download.manager.retention</command> to <command>1</command>,
and both <command>browser.sessionstore.privacy_level</command> and
<command>network.cookie.lifetimePolicy</command> to <command>2</command>.  Many
of these preferences are likely redundant with
<command>browser.privatebrowsing.autostart</command> enabled, but we have not
done the auditing work to ensure that yet.

   </blockquote>
   <blockquote>

For more details on disk leak bugs and enhancements, see the <ulink
url="https://trac.torproject.org/projects/tor/query?keywords=~tbb-disk-leak&amp;status=!closed">tbb-disk-leak tag in our bugtracker</ulink>
   </blockquote>
   </sect3>
  </sect2>
  <sect2 id="app-data-isolation">
   <title>Application Data Isolation</title>
   <para>

Tor Browser MUST NOT cause any information to be written outside of the bundle
directory. This is to ensure that the user is able to completely and
safely remove it without leaving other traces of Tor usage on their computer.

   </para>
   <para>

To ensure Tor Browser directory isolation, we set
<command>browser.download.useDownloadDir</command>,
<command>browser.shell.checkDefaultBrowser</command>, and
<command>browser.download.manager.addToRecentDocs</command>. We also set the
$HOME environment variable to be the Tor Browser extraction directory.
   </para>

  </sect2>
  <sect2 id="identifier-linkability">
   <title>Cross-Origin Identifier Unlinkability</title>
   <para>

The Cross-Origin Identifier Unlinkability design requirement is satisfied
through first party isolation of all browser identifier sources. First party
isolation means that all identifier sources and browser state are scoped
(isolated) using the URL bar domain. This scoping is performed in
combination with any additional third party scope. When first party isolation
is used with explicit identifier storage that already has a constrained third
party scope (such as cookies and DOM storage), this approach is
referred to as "double-keying".

   </para>
   <para>

The benefit of this approach comes not only in the form of reduced
linkability, but also in terms of simplified privacy UI. If all stored browser
state and permissions become associated with the URL bar origin, the six or
seven different pieces of privacy UI governing these identifiers and
permissions can become just one piece of UI. For instance, a window that lists
the URL bar origin for which browser state exists, possibly with a
context-menu option to drill down into specific types of state or permissions.
An example of this simplification can be seen in Figure 1.

   </para>
   <figure><title>Improving the Privacy UI</title>
    <mediaobject>
      <imageobject>
       <imagedata align="center" fileref="NewCookieManager.png"/>
      </imageobject>
    </mediaobject>
    <caption> <para/>

This example UI is a mock-up of how isolating identifiers to the URL bar
domain can simplify the privacy UI for all data - not just cookies. Once
browser identifiers and site permissions operate on a URL bar basis, the same
privacy window can represent browsing history, DOM Storage, HTTP Auth, search
form history, login values, and so on within a context menu for each site.

</caption>
   </figure>

 <sect3>
  <title>Identifier Unlinkability Defenses in the Tor Browser</title>
   <para>

Unfortunately, many aspects of browser state can serve as identifier storage,
and no other browser vendor or standards body had invested the effort to
enumerate or otherwise deal with these vectors for third party tracking. As
such, we have had to enumerate and isolate these identifier sources on a
piecemeal basis. This has gotten better lately with Mozilla stepping up and
helping us with uplifting our patches, and with contributing own ones where we
lacked proper fixes. However, we are not done yet with our unlinkability defense
as new identifier sources are still getting added to the web platform. Here is
the list that we have discovered and dealt with to date:

   </para>
   <orderedlist>
    <listitem><command>Cookies</command>
     <para><command>Design Goal:</command>

All cookies MUST be double-keyed to the URL bar origin and third-party
origin.

     </para>
     <para><command>Implementation Status:</command>

Double-keying cookies should just work by setting <command>privacy.firstparty.isolate
</command> to <command>true</command>. However,
<ulink url="https://trac.torproject.org/projects/tor/ticket/21905">we have not
audited that</ulink> yet and there is still the
<ulink url="https://trac.torproject.org/projects/tor/ticket/10353">UI part
missing for managing cookies in Private Browsing Mode</ulink>. We therefore
opted to keep third-party cookies disabled for now by setting
<command>network.cookie.cookieBehavior</command> to <command>1</command>.

     </para>
    </listitem>
    <listitem><command>Cache</command>
      <para><command>Design Goal:</command>
        All cache entries MUST be isolated to the URL bar domain.
      </para>
      <para><command>Implementation Status:</command>
We isolate the content and image cache to the URL bar domain by setting
<command>privacy.firstparty.isolate</command> to <command>true</command>.

      </para>
      <para>
Furthermore there is the Cache API (CacheStorage). That one is currently not
available in Tor Browser as we do not allow third party cookies and are in
Private Browsing Mode by default.
      </para>
      <para>
Finally, we have the asm.js cache. The cache entry of the sript is (among
others things, like type of CPU, build ID, source characters of the asm.js
module etc.) keyed <ulink url="https://blog.mozilla.org/luke/2014/01/14/asm-js-aot-compilation-and-startup-performance/">to the origin of the script</ulink>.
Lacking a good solution for binding it to the URL bar domain instead we decided
to disable asm.js for the time being by setting
<command>javascript.options.asmjs</command> to <command>false</command>. It
remains to be seen whether keying the cache entry e.g. to the source characters
of the asm.js module helps to avoid using it for cross-origin tracking of users.
We did not investigate that yet.
      </para>
    </listitem>
    <listitem><command>HTTP Authentication</command>
      <para>

HTTP Authorization headers can be used to encode <ulink
url="http://jeremiahgrossman.blogspot.com/2007/04/tracking-users-without-cookies.html">silent
third party tracking identifiers</ulink>. To prevent this, we set
<command>privacy.firstparty.isolate</command> to <command>true</command>.

      </para>
    </listitem>
    <listitem><command>DOM Storage</command>
      <para>

DOM storage for third party domains MUST be isolated to the URL bar domain,
to prevent linkability between sites. We achieve this by setting
<command>privacy.firstparty.isolate</command> to <command>true</command>.

      </para>
    </listitem>
    <listitem><command>Flash cookies</command>
      <para><command>Design Goal:</command>

Users should be able to click-to-play flash objects from trusted sites. To
make this behavior unlinkable, we wish to include a settings file for all
platforms that disables flash cookies using the <ulink
url="https://www.macromedia.com/support/documentation/en/flashplayer/help/settings_manager03.html">Flash
settings manager</ulink>.

      </para>
      <para><command>Implementation Status:</command>

We are currently <ulink
url="https://trac.torproject.org/projects/tor/ticket/3974">having
difficulties</ulink> causing Flash player to use this settings
file on Windows, so Flash remains difficult to enable.

      </para>
    </listitem>
    <listitem><command>SSL+TLS session resumption</command>
      <para><command>Design Goal:</command>

TLS session resumption tickets and SSL Session IDs MUST be limited to the URL
bar domain.

      </para>
      <para><command>Implementation Status:</command>

We disable TLS Session Tickets and SSL Session IDs by
setting <command>security.ssl.disable_session_identifiers</command> to
<command>true</command>.
To compensate for the increased round trip latency from disabling
these performance optimizations, we also enable
<ulink url="https://tools.ietf.org/html/draft-bmoeller-tls-falsestart-00">TLS
False Start</ulink> via the Firefox Pref
<command>security.ssl.enable_false_start</command>.
However, URL bar domain isolation should be working both for session tickets and
session IDs but we <ulink url="https://trac.torproject.org/projects/tor/ticket/17252">
have not verified that yet</ulink>.

      </para>
    </listitem>
    <listitem><command>Tor circuit and HTTP connection linkability</command>
      <para><command>Design Goal:</command>

Tor circuits and HTTP connections from a third party in one URL bar origin
MUST NOT be reused for that same third party in another URL bar origin.

      </para>
      <para><command>Implementation Status:</command>

The isolation functionality is provided by a Torbutton component that <ulink
url="https://gitweb.torproject.org/torbutton.git/tree/src/components/domain-isolator.js">sets
the SOCKS username and password for each request</ulink>. The Tor client has
logic to prevent connections with different SOCKS usernames and passwords from
using the same Tor circuit. Firefox has existing logic to ensure that
connections with SOCKS proxies do not re-use existing HTTP Keep-Alive
connections unless the proxy settings match.
<ulink url="https://bugzilla.mozilla.org/show_bug.cgi?id=1200802">We extended
this logic</ulink> to cover SOCKS username and password authentication,
providing us with HTTP Keep-Alive unlinkability.

      </para>
      <para>

While the vast majority of web requests adheres to the circuit and connection
unlinkability requirement there are still corner cases we
<ulink url="https://gitweb.torproject.org/tor-browser.git/commit/?h=tor-browser-52.5.2esr-7.0-2&amp;id=8661822237c56d543d5c9117c8a4708c402a110f">
  need to treat separately</ulink> or that
<ulink href="https://bugs.torproject.org/22343">lack a fix altogether</ulink>.
      </para>
    </listitem>
    <listitem><command>SharedWorkers</command>
      <para>

<ulink
url="https://developer.mozilla.org/en-US/docs/Web/API/SharedWorker">SharedWorkers</ulink>
are a special form of JavaScript Worker threads that have a shared scope between
all threads from the same Javascript origin. They MUST be isolated to the URL
bar domain. I.e. a SharedWorker launched from a third party from one URL bar
domain MUST NOT have access to the objects created by that same third party
loaded under another URL bar domain. This functionality is provided by setting
<command>privacy.firstparty.isolate</command> to <command>true</command>.

      </para>
    </listitem>
    <listitem><command>blob: URIs (URL.createObjectURL)</command>
      <para>

The <ulink
url="https://developer.mozilla.org/en-US/docs/Web/API/URL/createObjectURL">URL.createObjectURL</ulink>
API allows a site to load arbitrary content into a random UUID that is stored
in the user's browser, and this content can be accessed via a URL of the form
<command>blob:UUID</command> from any other content element anywhere on the
web. While this UUID value is neither under control of the site nor
predictable, it can still be used to tag a set of users that are of high
interest to an adversary.

      </para>
      <para>

URIs created with URL.createObjectURL MUST be limited in scope to the first
party URL bar domain that created them. We provide the isolation in Tor
Browser by setting <command>privacy.firstparty.isolate</command> to
<command>true</command>.

      </para>
    </listitem>
    <listitem><command>SPDY and HTTP/2</command>
      <para><command>Design Goal:</command>

SPDY and HTTP/2 connections MUST be isolated to the URL bar domain. Furthermore,
all associated means that could be used for cross-domain user tracking (alt-svc
headers come to mind) MUST adhere to this design principle as well.

      </para>
      <para><command>Implementation status:</command>

SPDY and HTTP/2 are currently disabled by setting the
Firefox preferences <command>network.http.spdy.enabled</command>,
<command>network.http.spdy.enabled.v2</command>,
<command>network.http.spdy.enabled.v3</command>,
<command>network.http.spdy.enabled.v3-1</command>,
<command>network.http.spdy.enabled.http2</command>,
<command>network.http.spdy.enabled.http2draft</command>,
<command>network.http.altsvc.enabled</command>, and
<command>network.http.altsvc.oe</command> to <command>false</command>.

      </para>
    </listitem>
    <listitem><command>Automated cross-origin redirects</command>
      <para><command>Design Goal:</command>

To prevent attacks aimed at subverting the Cross-Origin Identifier
Unlinkability <link linkend="privacy">privacy requirement</link>, the browser
MUST NOT store any identifiers (cookies, cache, DOM storage, HTTP auth, etc)
for cross-origin redirect intermediaries that do not prompt for user input.
For example, if a user clicks on a bit.ly URL that redirects to a
doubleclick.net URL that finally redirects to a cnn.com URL, only cookies from
cnn.com should be retained after the redirect chain completes.

      </para>
      <para>

Non-automated redirect chains that require user input at some step (such as
federated login systems) SHOULD still allow identifiers to persist.

      </para>
      <para><command>Implementation status:</command>

There are numerous ways for the user to be redirected, and the Firefox API
support to detect each of them is poor. We have a <ulink
url="https://trac.torproject.org/projects/tor/ticket/3600">trac bug
open</ulink> to implement what we can.

      </para>
    </listitem>
    <listitem><command>window.name</command>
      <para>

<ulink
url="https://developer.mozilla.org/En/DOM/Window.name">window.name</ulink> is
a magical DOM property that for some reason is allowed to retain a persistent value
for the lifespan of a browser tab. It is possible to utilize this property for
<ulink url="https://www.thomasfrank.se/sessionvars.html">identifier
storage</ulink>.

      </para>
      <para>

In order to eliminate non-consensual linkability but still allow for sites
that utilize this property to function, we reset the window.name property of
tabs in Torbutton every time we encounter a blank Referer. This behavior
allows window.name to persist for the duration of a click-driven navigation
session, but as soon as the user enters a new URL or navigates between
HTTPS/HTTP schemes, the property is cleared.

      </para>
    </listitem>
    <listitem><command>Auto form-fill</command>
      <para>

We disable the password saving functionality in the browser as part of our
<link linkend="disk-avoidance">Disk Avoidance</link> requirement. However,
since users may decide to re-enable disk history records and password saving,
we also set the <ulink
url="http://kb.mozillazine.org/Signon.autofillForms">signon.autofillForms</ulink>
preference to false to prevent saved values from immediately populating
fields upon page load. Since JavaScript can read these values as soon as they
appear, setting this preference prevents automatic linkability from stored passwords.

      </para>
    </listitem>
    <listitem><command>HSTS and HPKP supercookies</command>
      <para>

An extreme (but not impossible) attack to mount is the creation of <ulink
  url="https://www.leviathansecurity.com/blog/archives/12-The-Double-Edged-Sword-of-HSTS-Persistence-and-Privacy.html">HSTS</ulink>
<ulink url="https://www.radicalresearch.co.uk/lab/hstssupercookies/">
supercookies</ulink>. Since HSTS effectively stores one bit of information per domain
name, an adversary in possession of numerous domains can use them to construct
cookies based on stored HSTS state.

      </para>
      <para>

HPKP provides <ulink url="https://zyan.scripts.mit.edu/presentations/toorcon2015.pdf">
a mechanism for user tracking</ulink> across domains as well. It allows abusing the
requirement to provide a backup pin and the option to report a pin validation
failure. In a tracking scenario every user gets a unique SHA-256 value serving
as backup pin. This value is sent back after (deliberate) pin validation failures
working in fact as a cookie.

      </para>
      <para><command>Design Goal:</command>

HSTS and HPKP MUST be isolated to the URL bar domain.

      </para>
      <para><command>Implementation Status:</command>

Currently, HSTS and HPKP state is both cleared by <link linkend="new-identity">New Identity</link>,
but we don't defend against the creation and usage of any of these supercookies
between <command>New Identity</command> invocations.

      </para>
    </listitem>
    <listitem><command>Broadcast Channels</command>
       <para>

The BroadcastChannel API allows cross-site communication within the same
origin. However, to avoid cross-origin linkability broadcast channels MUST
instead be isolated to the URL bar domain.

      </para>
      <para>

We provide the isolation in Tor Browser by setting
<command>privacy.firstparty.isolate</command> to <command>true</command>.

      </para>
     </listitem>
     <listitem><command>OCSP</command>
       <para>

OCSP requests go to Certfication Authorities (CAs) to check for revoked
certificates. They are sent once the browser is visiting a website via HTTPS and
no cached results are available. Thus, to avoid information leaks, e.g. to exit
relays, OCSP requests MUST go over the same circuit as the HTTPS request causing
them and MUST therefore be isolated to the URL bar domain. The resulting cache
entries MUST be bound to the URL bar domain as well. This functionality is
provided by setting <command>privacy.firstparty.isolate</command> to
<command>true</command>.

       </para>
    </listitem>
    <listitem><command>Favicons</command>
      <para><command>Design Goal:</command>

When visiting a website its favicon is fetched via a request originating from
the browser itself (similar to the OCSP mechanism mentioned in the previous
section). Those requests MUST be isolated to the URL bar domain.

      </para>
      <para><command>Implemetation Status:</command>

Favicon requests are isolated to the URL bar domain by setting
<command>privacy.firstparty.isolate</command> to <command>true</command>.
However, we need an additional
<ulink url="https://gitweb.torproject.org/tor-browser.git/commit/?h=tor-browser-52.5.2esr-7.0-2&amp;id=eaa22334adaf8f79544ee4318982e5f4990c1a6f">Firefox patch</ulink>
to take care of favicons in tab list menuitems.
      </para>
    </listitem>
    <listitem><command>mediasource: URIs and MediaStreams</command>
      <para>

Much like blob URLs, mediasource: URIs and MediaStreams can be used to tag
users. Therefore, mediasource: URIs and MediaStreams MUST be isolated to the URL bar domain.
This functionality is provided by setting <command>privacy.firstparty.isolate</command>
to <command>true</command>.
      </para>
    </listitem>
    <listitem><command>Speculative and prefetched connections</command>
      <para>

Firefox provides the feature to <ulink url="https://www.igvita.com/2015/08/17/eliminating-roundtrips-with-preconnect/">connect speculatively</ulink> to
remote hosts if that is either indicated in the HTML file (e.g. by
<ulink url="https://w3c.github.io/resource-hints/">link
rel="preconnect" and rel="prefetch"</ulink>) or otherwise deemed beneficial.

      </para>
      <para>

Firefox does not support rel="prerender", and Mozilla has disabled speculative
connections and rel="preconnect" usage where a proxy is used (see <ulink
url="https://trac.torproject.org/projects/tor/ticket/18762#comment:3"> comment
3 in bug 18762</ulink> for further details). Explicit prefetching via the
rel="prefetch" attribute is still performed, however.

      </para>
      <para>

All pre-loaded links and speculative connections MUST be isolated to the URL
bar domain, if enabled. This includes isolating both Tor circuit use, as well
as the caching and associate browser state for the prefetched resource.

      </para>
      <para>

For automatic speculative connects and rel="preconnect", we leave them
disabled as per the Mozilla default for proxy settings. However, if enabled,
speculative connects will be isolated to the proper first party Tor circuit by
the same mechanism as is used for HTTP Keep-Alive. This is true for rel="prefetch"
requests as well. For rel="preconnect", we set <command>privacy.firstparty.isolate</command>
to <command>true</command>. This isolation makes both preconnecting and cache
warming via rel="prefetch" ineffective for links to domains other than the
current URL bar domain. For links to the same domain as the URL bar domain,
the full cache warming benefit is obtained. As an optimization, any
preconnecting to domains other than the current URL bar domain can thus be
disabled (perhaps with the exception of frames), but we do not do this.
We allow these requests to proceed, but we isolate them.

      </para>
    </listitem>
    <listitem><command>Permissions API</command>
      <para>

The Permissions API allows a website to query the status of different
permissions. Although permissions are keyed to the origin, that is not enough to
alleviate cross-linkabilility concerns: the combined permission state could work
like an identifier given more and more permissions and their state being
accessible under this API.

      </para>
      <para><command>Design Goal:</command>

Permissions MUST be isolated to the URL bar domain.

      </para>
      <para><command>Implementation Status:</command>

Right now we provide a <ulink url="https://gitweb.torproject.org/tor-browser.git/commit/?h=tor-browser-52.5.2esr-7.0-2&amp;id=14374d30767f83923561084530b54c066bb661b4">Firefox patch</ulink> that makes sure permissions are isolated to the URL bar domain.

      </para>
    </listitem>
   </orderedlist>
   <para>
For more details on identifier linkability bugs and enhancements, see the <ulink
url="https://trac.torproject.org/projects/tor/query?keywords=~tbb-linkability&amp;status=!closed">tbb-linkability tag in our bugtracker</ulink>
  </para>
  </sect3>
  </sect2>
  <sect2 id="fingerprinting-linkability">
   <title>Cross-Origin Fingerprinting Unlinkability</title>

   <para>
Browser fingerprinting is the act of inspecting browser behaviors and features in
an attempt to differentiate and track individual users.
  </para>
  <para>

Fingerprinting attacks are typically broken up into passive and active
vectors. Passive fingerprinting makes use of any information the browser
provides automatically to a website without any specific action on the part of
the website. Active fingerprinting makes use of any information that can be
extracted from the browser by some specific website action, usually involving
JavaScript. Some definitions of browser fingerprinting also include
supercookies and cookie-like identifier storage, but we deal with those issues
separately in the <link linkend="identifier-linkability">preceding section on
identifier linkability</link>.

    </para>
    <para>

For the most part, however, we do not differentiate between passive or active
fingerprinting sources, since many active fingerprinting mechanisms are very
rapid, and can be obfuscated or disguised as legitimate functionality.

   </para>
    <para>

Instead, we believe fingerprinting can only be rationally addressed if we
understand where the problem comes from, what sources of issues are the most
severe, what types of defenses are suitable for which sources, and have a
consistent strategy for designing defenses that maximizes our ability to study
defense efficacy. The following subsections address these issues from a high
level, and we then conclude with a list of our current specific defenses.

    </para>

   <sect3 id="fingerprinting-scope">
    <title>Sources of Fingerprinting Issues</title>
    <para>

All browser fingerprinting issues arise from one of four primary sources:
end-user configuration details, device and hardware characteristics, operating
system vendor and version differences, and browser vendor and version
differences. Additionally, user behavior itself provides one more source of
potential fingerprinting.

    </para>
    <para>

In order to help prioritize and inform defenses, we now list these sources in
order from most severe to least severe in terms of the amount of information
they reveal, and describe them in more detail.

    </para>
    <orderedlist>
     <listitem><command>End-user Configuration Details</command>
      <para>

End-user configuration details are by far the most severe threat to
fingerprinting, as they will quickly provide enough information to uniquely
identify a user. We believe it is essential to avoid exposing platform
configuration details to website content at all costs. We also discourage
excessive fine-grained customization of Tor Browser by minimizing and
aggregating user-facing privacy and security options, as well as by
discouraging the use of additional plugins and addons. When it is necessary to
expose configuration details in the course of providing functionality, we
strive to do so only on a per-site basis via site permissions, to avoid
linkability.

     </para>
    </listitem>
     <listitem><command>Device and Hardware Characteristics</command>
      <para>

Device and hardware characteristics can be determined in three ways: they can
be reported explicitly by the browser, they can be inferred through browser
functionality, or they can be extracted through statistical measurements of
system performance. We are most concerned with the cases where this
information is either directly reported or can be determined via a single use
of an API or feature, and prefer to either alter functionality to prevent
exposing the most variable aspects of these characteristics, place such
features behind site permissions, or disable them entirely.

      </para>
      <para>

On the other hand, because statistical inference of system performance
requires many iterations to achieve accuracy in the face of noise and
concurrent activity, we are less concerned with this mechanism of extracting
this information. We also expect that reducing the resolution of JavaScript's
time sources will significantly increase the duration of execution required to
extract accurate results, and thus make statistical approaches both
unattractive and highly noticeable due to excessive resource consumption.

      </para>
     </listitem>
     <listitem><command>Operating System Vendor and Version Differences</command>
      <para>

Operating system vendor and version differences permeate many different
aspects of the browser. While it is possible to address these issues with some
effort, the relative lack of diversity in operating systems causes us to
primarily focus our efforts on passive operating system fingerprinting
mechanisms at this point in time. For the purposes of protecting user
anonymity, it is not strictly essential that the operating system be
completely concealed, though we recognize that it is useful to reduce this
differentiation ability where possible, especially for cases where the
specific version of a system can be inferred.

      </para>
     </listitem>
     <listitem><command>User Behavior</command>
      <para>

While somewhat outside the scope of browser fingerprinting, for completeness
it is important to mention that users themselves theoretically might be
fingerprinted through their behavior while interacting with a website. This
behavior includes e.g. keystrokes, mouse movements, click speed, and writing
style. Basic vectors such as keystroke and mouse usage fingerprinting can be
mitigated by altering JavaScript's notion of time. More advanced issues like
writing style fingerprinting are the domain of <ulink
url="https://github.com/psal/anonymouth/blob/master/README.md">other tools</ulink>.

      </para>
     </listitem>
     <listitem><command>Browser Vendor and Version Differences</command>
      <para>

Due to vast differences in feature set and implementation behavior even
between different (<ulink url="https://tsyrklevich.net/2014/10/28/abusing-strict-transport-security/">minor</ulink>)
versions of the same browser, browser vendor and version differences are simply
not possible to conceal in any realistic way. It is only possible to minimize
the differences among different installations of the same browser vendor and
version. We make no effort to mimic any other major browser vendor, and in fact
most of our fingerprinting defenses serve to differentiate Tor Browser users
from normal Firefox users. Because of this, any study that lumps browser vendor
and version differences into its analysis of the fingerprintability of a
population is largely useless for evaluating either attacks or defenses.
Unfortunately, this includes popular large-scale studies such as <ulink
url="https://panopticlick.eff.org/">Panopticlick</ulink> and <ulink
url="https://amiunique.org/">Am I Unique</ulink>.

      </para>
     </listitem>
   </orderedlist>
  </sect3>
   <sect3 id="fingerprinting-defenses-general">
    <title>General Fingerprinting Defenses</title>
    <para>

To date, the Tor Browser team has concerned itself only with developing
defenses for APIs that have already been standardized and deployed. Once an
API or feature has been standardized and widely deployed, defenses to the
associated fingerprinting issues tend to have only a few options available to
compensate for the lack of up-front privacy design. In our experience, so far
these options have been limited to value spoofing, subsystem modification or
reimplementation, virtualization, site permissions, and feature removal. We
will now describe these options and the fingerprinting sources they tend to
work best with.

    </para>
  <orderedlist>
   <listitem><command>Value Spoofing</command>
     <para>

Value spoofing can be used for simple cases where the browser provides some
aspect of the user's configuration details, devices, hardware, or operating
system directly to a website. It becomes less useful when the fingerprinting
method relies on behavior to infer aspects of the hardware or operating system,
rather than obtain them directly.

     </para>
   </listitem>
   <listitem><command>Subsystem Modification or Reimplementation</command>
   <para>

In cases where simple spoofing is not enough to properly conceal underlying
device characteristics or operating system details, the underlying subsystem
that provides the functionality for a feature or API may need to be modified
or completely reimplemented. This is most common in cases where customizable
or version-specific aspects of the user's operating system are visible through
the browser's featureset or APIs, usually because the browser directly exposes
OS-provided implementations of underlying features. In these cases, such
OS-provided implementations must be replaced by a generic implementation, or
at least modified by an implementation wrapper layer that makes effort to
conceal any user-customized aspects of the system.

   </para>
  </listitem>
  <listitem><command>Virtualization</command>
   <para>

Virtualization is needed when simply reimplementing a feature in a different
way is insufficient to fully conceal the underlying behavior. This is most
common in instances of device and hardware fingerprinting, but since the
notion of time can also be virtualized, virtualization also can apply to any
instance where an accurate measurement of wall clock time is required for a
fingerprinting vector to attain high accuracy.

   </para>
  </listitem>
  <listitem><command>Site Permissions</command>
   <para>

In the event that reimplementation or virtualization is too expensive in terms
of performance or engineering effort, and the relative expected usage of a
feature is rare, site permissions can be used to prevent the usage of a
feature for cross-site tracking. Unfortunately, site permissions become less
effective once a feature is already widely overused and abused by many
websites, since warning fatigue typically sets in for most users after just a
few permission requests.

   </para>
  </listitem>
  <listitem><command>Feature or Functionality Removal</command>
   <para>

Due to the current bias in favor of invasive APIs that expose the maximum
amount of platform information, some features and APIs are simply not
salvageable in their current form. When such invasive features serve only a
narrow domain or use case, or when there are alternate ways of accomplishing
the same task, these features and/or certain aspects of their functionality
may be simply removed.

   </para>
  </listitem>
  </orderedlist>
  </sect3>
  <sect3>
   <title>Strategies for Defense: Randomization versus Uniformity</title>
    <para>

When applying a form of defense to a specific fingerprinting vector or source,
there are two general strategies available: either the implementation for all
users of a single browser version can be made to behave as uniformly as
possible, or the user agent can attempt to randomize its behavior so that
each interaction between a user and a site provides a different fingerprint.

    </para>
    <para>

Although <ulink url="https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/tr1-1.pdf">
some research suggests</ulink> that randomization can be effective, so far
striving for uniformity has generally proved to be a better strategy for Tor
Browser for the following reasons:

    </para>

   <orderedlist>
     <listitem><command>Evaluation and measurement difficulties</command>
      <para>

The fact that randomization causes behaviors to differ slightly with every
site visit makes it appealing at first glance, but this same property makes it
very difficult to objectively measure its effectiveness. By contrast, an
implementation that strives for uniformity is very simple to evaluate. Despite
their current flaws, a properly designed version of <ulink
url="https://panopticlick.eff.org/">Panopticlick</ulink> or <ulink
url="https://amiunique.org/">Am I Unique</ulink> could report the entropy and
uniqueness rates for all users of a single user agent version, without the
need for complicated statistics about the variance of the measured behaviors.

      </para>
      <para>

Randomization (especially incomplete randomization) may also provide a false
sense of security. When a fingerprinting attempt makes naive use of randomized
information, a fingerprint will appear unstable, but may not actually be
sufficiently randomized to impede a dedicated adversary. Sophisticated
fingerprinting mechanisms may either ignore randomized information, or
incorporate knowledge of the distribution and range of randomized values into
the creation of a more stable fingerprint (by either removing the randomness,
modeling it, or averaging it out).

      </para>
     </listitem>
    <listitem><command>Randomization is not a shortcut</command>
     <para>

While many end-user configuration details that the browser currently exposes
may be safely replaced by false information, randomization of these details
must be just as exhaustive as an approach that seeks to make these behaviors
uniform. When confronting either strategy, the adversary can still make use of
any details which have not been altered to be either sufficiently uniform or
sufficiently random.

     </para>
     <para>

Furthermore, the randomization approach seems to break down when it is applied
to deeper issues where underlying system functionality is directly exposed. In
particular, it is not clear how to randomize the capabilities of hardware
attached to a computer in such a way that it either convincingly behaves like
other hardware, or such that the exact properties of the hardware that vary
from user to user are sufficiently randomized. Similarly, truly concealing
operating system version differences through randomization may require
multiple reimplementations of the underlying operating system functionality to
ensure that every operating system version is covered by the range of possible
behaviors.

     </para>
     </listitem>
     <listitem><command>Usability issues</command>
      <para>

When randomization is introduced to features that affect site behavior, it can
be very distracting for this behavior to change between visits of a given
site. For the simplest cases, this will lead to minor visual nuisances.
However, when this information affects reported functionality or hardware
characteristics, sometimes a site will function one way on one visit, and
another way on a subsequent visit.

      </para>
     </listitem>
     <listitem><command>Performance costs</command>

      <para>

Randomizing involves performance costs. This is especially true if the
fingerprinting surface is large (like in a modern browser) and one needs more
elaborate randomizing strategies (including randomized virtualization) to
ensure that the randomization fully conceals the true behavior. Many calls to
a cryptographically secure random number generator during the course of a page
load will both serve to exhaust available entropy pools, as well as lead to
increased computation while loading a page.

      </para>
     </listitem>
     <listitem><command>Increased vulnerability surface</command>
      <para>

Improper randomization might introduce a new fingerprinting vector, as the
process of generating the values for the fingerprintable attributes could be
itself susceptible to side-channel attacks, analysis, or exploitation.

      </para>
     </listitem>
  </orderedlist>
  </sect3>
  <sect3 id="fingerprinting-defenses">
   <title>Specific Fingerprinting Defenses in the Tor Browser</title>
   <para>

The following defenses are listed roughly in order of most severe
fingerprinting threat first. This ordering is based on the above intuition
that user configurable aspects of the computer are the most severe source of
fingerprintability, followed by device characteristics and hardware, and then
finally operating system vendor and version information.

   </para>

   <para>

Where our actual implementation differs from an ideal solution, we separately
describe our <command>Design Goal</command> and our <command>Implementation
Status</command>.

   </para>
   <orderedlist>
    <listitem><command>Plugins</command>
     <para>

Plugins add to fingerprinting risk via two main vectors: their mere presence
in window.navigator.plugins (because they are optional, end-user installed
third party software), as well as their internal functionality.

     </para>
     <para><command>Design Goal:</command>

All plugins that have not been specifically audited or sandboxed MUST be
disabled. To reduce linkability potential, even sandboxed plugins SHOULD NOT
be allowed to load objects until the user has clicked through a click-to-play
barrier. Additionally, version information SHOULD be reduced or obfuscated
until the plugin object is loaded. For Flash, we wish to <ulink
url="https://trac.torproject.org/projects/tor/ticket/3974">provide a
settings.sol file</ulink> to disable Flash cookies, and to restrict P2P
features that are likely to bypass proxy settings. We'd also like to restrict
access to fonts and other system information (such as IP address and MAC
address) in such a sandbox.

     </para>
     <para><command>Implementation Status:</command>

Currently, we entirely disable all plugins in Tor Browser. However, as a
compromise due to the popularity of Flash, we allow users to re-enable Flash,
and flash objects are blocked behind a click-to-play barrier that is available
only after the user has specifically enabled plugins. Flash is the only plugin
available, the rest are entirely
blocked from loading by the Firefox patches mentioned in the <link
linkend="proxy-obedience">Proxy Obedience
section</link>. We also set the Firefox
preference <command>plugin.expose_full_path</command> to
<command>false</command>, to avoid leaking plugin installation information.

     </para>
    </listitem>
    <listitem><command>HTML5 Canvas Image Extraction</command>
     <para>

After plugins and plugin-provided information, we believe that the <ulink
url="https://developer.mozilla.org/en-US/docs/HTML/Canvas">HTML5
Canvas</ulink> is the single largest fingerprinting threat browsers face
today. <ulink
url="https://cseweb.ucsd.edu/~hovav/dist/canvas.pdf">
Studies</ulink> <ulink url="https://securehomes.esat.kuleuven.be/~gacar/persistent/the_web_never_forgets.pdf">show</ulink> that the Canvas can provide an easy-access fingerprinting
target: The adversary simply renders WebGL, font, and named color data to a
Canvas element, extracts the image buffer, and computes a hash of that image
data. Subtle differences in the video card, font packs, and even font and
graphics library versions allow the adversary to produce a stable, simple,
high-entropy fingerprint of a computer. In fact, the hash of the rendered
image can be used almost identically to a tracking cookie by the web server.

     </para>
     <para>

In some sense, the canvas can be seen as the union of many other
fingerprinting vectors. If WebGL is normalized through software rendering,
system colors were standardized, and the browser shipped a fixed collection of
fonts (see later points in this list), it might not be necessary to create a
canvas permission. However, until then, to reduce the threat from this vector,
we have patched Firefox to <ulink url="https://gitweb.torproject.org/tor-browser.git/commit/?h=tor-browser-52.5.2esr-7.0-2&amp;id=196354d7951a48b4e6f5309d2a8e46962fff9d5f">prompt before returning valid image data</ulink> to the Canvas APIs,
and for access to isPointInPath and related functions. Moreover, we put media
streams on a canvas behind the site permission in that patch as well.
If the user hasn't previously allowed the site in the URL bar to access Canvas
image data, pure white image data is returned to the JavaScript APIs.
Extracting canvas image data by third parties is not allowed, though.

     </para>
     <para>
     </para>
    </listitem>
    <listitem><command>Open TCP Port and Local Network Fingerprinting</command>
     <para>

In Firefox, by using either WebSockets or XHR, it is possible for remote
content to <ulink url="http://www.andlabs.org/tools/jsrecon.html">enumerate
the list of TCP ports open on 127.0.0.1</ulink>, as well as on any other
machines on the local network. In other browsers, this can be accomplished by
DOM events on image or script tags. This open vs filtered vs closed port list
can provide a very unique fingerprint of a machine, because it essentially
enables the detection of many different popular third party applications and
optional system services (Skype, Bitcoin, Bittorrent and other P2P software,
SSH ports, SMB and related LAN services, CUPS and printer daemon config ports,
mail servers, and so on). It is also possible to determine when ports are
closed versus filtered/blocked (and thus probe custom firewall configuration).

     </para>

	 <para>

In Tor Browser, we prevent access to 127.0.0.1/localhost by ensuring that even
these requests are still sent by Firefox to our SOCKS proxy (ie we set
<command>network.proxy.no_proxies_on</command> to the empty string). The local
Tor client then rejects them, since it is configured to proxy for internal IP
addresses by default. Access to the local network is forbidden via the same
mechanism. We also disable the WebRTC API as mentioned previously, since even
if it were usable over Tor, it still currently provides the local IP address
and associated network information to websites. Additionally, we
<ulink url="https://gitweb.torproject.org/tor-browser.git/commit/?h=tor-browser-52.5.2esr-7.0-2&amp;id=13baf9df4b47bd13bb7da045048ed4339615ac03">
rip out</ulink> the option to collect local IP addresses via the
NetworkInfoService.

     </para>

    </listitem>
     <listitem><command>Invasive Authentication Mechanisms (NTLM and SPNEGO)</command>
     <para>

Both NTLM and SPNEGO authentication mechanisms can leak the hostname, and in
some cases the current username. The only reason why these aren't a more
serious problem is that they typically involve user interaction, and likely
aren't an attractive vector for this reason. However, because it is not clear
if certain carefully-crafted error conditions in these protocols could cause
them to reveal machine information and still fail silently prior to the
password prompt, these authentication mechanisms should either be disabled, or
placed behind a site permission before their use. We simply disable them
<ulink url="https://gitweb.torproject.org/tor-browser.git/commit/?h=tor-browser-52.5.2esr-7.0-2&amp;id=fe465944545a76287842321175cc7713091e77b1">with a patch</ulink>.

     </para>
    </listitem>
   <listitem><command>USB Device ID Enumeration via the GamePad API</command>
     <para>

The <ulink
url="https://developer.mozilla.org/en-US/docs/Web/Guide/API/Gamepad">GamePad
API</ulink> provides web pages with the <ulink
url="https://dvcs.w3.org/hg/gamepad/raw-file/default/gamepad.html#widl-Gamepad-id">USB
device id, product id, and driver name</ulink> of all connected game
controllers, as well as detailed information about their capabilities.
    </para>
    <para>

It's our opinion that this API needs to be completely redesigned to provide an
abstract notion of a game controller rather than offloading all of the
complexity associated with handling specific game controller models to web
content authors. For systems without a game controller, a standard controller
can be virtualized through the keyboard, which will serve to both improve
usability by normalizing user interaction with different games, as well as
eliminate fingerprinting vectors. Barring that, this API should be behind a
site permission in Private Browsing Modes. For now though, we simply disable
it via the pref <command>dom.gamepad.enabled</command>.

     </para>
    </listitem>
    <listitem><command>Fonts</command>
     <para>

According to the Panopticlick study, fonts provide the most linkability when
they are available as an enumerable list in file system order, via either the
Flash or Java plugins. However, it is still possible to use CSS and/or
JavaScript to query for the existence of specific fonts. With a large enough
pre-built list to query, a large amount of fingerprintable information may
still be available, especially given that additional fonts often end up
installed by third party software and for multilingual support.

     </para>

     <para><command>Design Goal:</command>Font-based fingerprinting MUST be rendered ineffective</para>

     <para><command>Implementation Status:</command>

We <ulink url="https://trac.torproject.org/projects/tor/ticket/13313">investigated
</ulink>shipping a predefined set of fonts to all of our users allowing only
those fonts to be used by websites at the exclusion of system fonts. We are
currently following this approach, which has been <ulink url="https://www.bamsoftware.com/papers/fontfp.pdf">
suggested</ulink> <ulink url="https://cseweb.ucsd.edu/~hovav/dist/canvas.pdf">by
researchers</ulink> previously. This defense is available for all three
supported platforms: Windows, macOS, and Linux, although the implementations
vary in detail.

     </para>
     <para>

For Windows and macOS we use a preference, <command>font.system.whitelist</command>,
to restrict fonts being used to those in the whitelist. This functionality is
provided by setting <command>privacy.resistFingerprinting</command> to
<command>true</command>.
The whitelist for Windows and macOS contains both a set of
<ulink url="https://www.google.com/get/noto">Noto fonts</ulink> which we bundle
and fonts provided by the operating system. For Linux systems we only bundle
fonts and <ulink url="https://gitweb.torproject.org/builders/tor-browser-bundle.git/commit/?id=b88443f6d8af62f763b069eb15e008a46d9b468a">
deploy </ulink> a <command>fonts.conf</command> file to restrict the browser to
use those fonts exclusively. In addition to that we set the <command>font.name*
</command> preferences for macOS and Linux to make sure that a given code point
is always displayed with the same font. This is not guaranteed even if we bundle
all the fonts Tor Browser uses as it can happen that fonts are loaded in a
different order on different systems. Setting the above mentioned preferences
works around this issue by specifying the font to use explicitely.

     </para>

     <para>

Allowing fonts provided by the operating system for Windows and macOS users is
currently a compromise between fingerprintability resistance and usability
concerns. We are still investigating the right balance between them and have
created a <ulink url="https://trac.torproject.org/projects/tor/ticket/18097">
ticket in our bug tracker</ulink> to summarize the current state of our defense
and future work that remains to be done.

     </para>
    </listitem>
    <listitem><command>Monitor, Widget, and OS Desktop Resolution</command>
     <para>

Both CSS and JavaScript have access to a lot of information about the screen
resolution, usable desktop size, OS widget size, toolbar size, title bar size,
and OS desktop widget sizing information that are not at all relevant to
rendering and serve only to provide information for fingerprinting. Since many
aspects of desktop widget positioning and size are user configurable, these
properties yield customized information about the computer, even beyond the
monitor size.

     </para>
     <para><command>Design Goal:</command>

Our design goal here is to reduce the resolution information down to the bare
minimum required for properly rendering inside a content window. We intend to
report all rendering information correctly with respect to the size and
properties of the content window, but report an effective size of 0 for all
border material, and also report that the desktop is only as big as the inner
content window. Additionally, new browser windows are sized such that their
content windows are one of a few fixed sizes based on the user's desktop
resolution. In addition, to further reduce resolution-based fingerprinting, we
are <ulink
url="https://trac.torproject.org/projects/tor/ticket/7256">investigating
zoom/viewport-based mechanisms</ulink> that might allow us to always report the
same desktop resolution regardless of the actual size of the content window,
and simply scale to make up the difference. As an alternative to zoom-based
solutions we are testing a
<ulink url="https://trac.torproject.org/projects/tor/ticket/14429">different
approach</ulink> in our alpha series that tries to round the browser window at
all times to a multiple 200x100 pixels. Regardless which solution we finally
pick, until it will be available the user should also be informed that
maximizing their windows can lead to fingerprintability under the current scheme.

     </para>
     <para><command>Implementation Status:</command>

We automatically resize new browser windows to a 200x100 pixel multiple based
on desktop resolution by backporting patches from
<ulink href="https://bugzilla.mozilla.org/show_bug.cgi?id=1330882">bug 1330882</ulink>
and setting <command>privacy.resistfingerprinting</command> to
<command>true</command>. To minimize the effect of the long tail of large
monitor sizes, we also cap the window size at 1000 pixels in each direction.
In addition to that we set <command>privacy.resistFingerprinting</command>
to <command>true</command> to use the client content window size for
window.screen, and to report a window.devicePixelRatio of 1.0. Similarly,
we use that preference to return content window relative points for DOM events.

We also force popups to open in new tabs (via
<command>browser.link.open_newwindow.restriction</command>), to avoid
full-screen popups inferring information about the browser resolution. In
addition, we prevent auto-maximizing on browser start, and inform users that
maximized windows are detrimental to privacy in this mode.

     </para>
    </listitem>

    <listitem><command>Display Media information</command>
     <para>

Beyond simple resolution information, a large amount of so-called "Media"
information is also exported to content. Even without JavaScript, CSS has
access to a lot of information about the device orientation, system theme
colors, and other desktop and display features that are not at all relevant to
rendering and also user configurable. Most of this
information comes from <ulink
url="https://developer.mozilla.org/en-US/docs/Web/Guide/CSS/Media_queries">CSS
Media Queries</ulink>, but Mozilla has exposed <ulink
url="https://developer.mozilla.org/en-US/docs/Web/CSS/color_value#System_Colors">several
user and OS theme defined color values</ulink> to CSS as well.

     </para>
     <para><command>Design Goal:</command>

A website MUST NOT be able infer anything that the user has configured about
their computer. Additionally, it SHOULD NOT be able to infer machine-specific
details such as screen orientation or type.

     </para>
     <para><command>Implementation Status:</command>

We set <command>ui.use_standins_for_native_colors</command> to <command>true
</command> and provide a <ulink url="https://gitweb.torproject.org/tor-browser.git/commit/?h=tor-browser-52.5.2esr-7.0-2&amp;id=9e84b962ae4e7369fcf13fdf3adb646877d48f1d">Firefox patch</ulink>
to report a fixed set of system colors to content window CSS, and prevent
detection of font smoothing on macOS with the help of
<command>privacy.resistFingerprinting</command> set to <command>true</command>.
We use the same preference, too, to always report landscape-primary for the
<ulink url="https://w3c.github.io/screen-orientation/">screen orientation</ulink>.

     </para>
    </listitem>
    <listitem><command>WebGL</command>
     <para>

WebGL is fingerprintable both through information that is exposed about the
underlying driver and optimizations, as well as through performance
fingerprinting.

     </para>
     <para>

Because of the large amount of potential fingerprinting vectors and the <ulink
url="https://www.contextis.com/resources/blog/webgl-new-dimension-browser-exploitation/">
previously unexposed vulnerability surface</ulink>, we deploy a similar strategy
against WebGL as for plugins. First, WebGL Canvases have click-to-play
placeholders (provided by NoScript), and do not run until authorized by the user.
Second, we obfuscate driver information by setting the Firefox preferences
<command>webgl.disable-extensions</command>,
<command>webgl.min_capability_mode</command>, and
<command>webgl.disable-fail-if-major-performance-caveat</command> to
<command>true</command> which reduces the information provided by the following
WebGL API calls: <command>getParameter()</command>,
<command>getSupportedExtensions()</command>, and <command>getExtension()</command>. Furthermore, WebGL2 is disabled by setting <command>webgl.enable-webgl2</command>
to <command>false</command>. To make the minimal WebGL mode usable we
additionally <ulink url="https://gitweb.torproject.org/tor-browser.git/commit/?h=tor-browser-52.5.2esr-7.0-2&amp;id=1acd0c7fae9121240401cf4a8f0e2b1f6fdb9827">
normalize its properties with a Firefox patch</ulink>.

     </para>
     <para>

Another option for WebGL might be to use software-only rendering, using a
library such as <ulink url="https://www.mesa3d.org/">Mesa</ulink>. The use of
such a library would avoid hardware-specific rendering differences.

     </para>
    </listitem>
    <listitem><command>MediaDevices API</command>

    <para>
The <ulink url="https://developer.mozilla.org/en-US/docs/Web/API/MediaDevices">
MediaDevices API</ulink> provides access to connected media input devices like
cameras and microphones, as well as screen sharing. In particular, it allows web
content to easily enumerate those devices with <command>
MediaDevices.enumerateDevices()</command>. This relies on WebRTC being compiled
in which we currently don't do. Nevertheless, we disable this feature for now as
a defense-in-depth by setting <command>media.peerconnection.enabled</command> and
<command>media.navigator.enabled</command> to <command>false</command>.
    </para>
    </listitem>

    <listitem><command>MIME Types</command>
     <para>

Which MIME Types are registered with an operating system depends to a great deal
on the application software and/or drivers a user chose to install. Web pages
can not only estimate the amount of MIME types registered by checking
<command>navigator.mimetypes.length</command>. Rather, they are even able to
test whether particular MIME types are available which can have a non-negligible
impact on a user's fingerprint. We prevent both of these information leaks by
setting <command>privacy.resistfingerprinting</command> to <command>true</command>.
    </para>
    </listitem>
    <listitem><command>Web Speech API</command>
      <para>

The Web Speech API consists of two parts: SpeechSynthesis (Text-to-Speech) and
SpeechRecognition (Asynchronous Speech Recognition). The latter is still
disabled in Firefox. However, the former is enabled by default and there is the
risk that <command>speechSynthesis.getVoices()</command> has access to
computer-specific speech packages making them available in an enumerable
fashion. Morevover, there are callbacks that would allow JavaScript to time how
long a phrase takes to be "uttered". To prevent both we set
<command>media.webspeech.synth.enabled</command> to <command>false</command>.

      </para>
    </listitem>

    <listitem><command>Touch API</command>
      <para>

Touch events are able to reveal the absolute screen coordinates of a device
which would defeat our approach to mitigate leaking the screen size as described
above. In order to prevent that we implemented two defenses: first we disable
the Touch API by setting <command>dom.w3c_touch_events.enabled</command> to
<command>false</command>. Second, for those user that really need or want to
have this API available we patched the code to give content-window related
coordinates back. Furthermore, we made sure that the touch area described by
<command>Touch.radiusX</command>, <command>Touch.radiusY</command>, and
<command>Touch.rotationAngle</command> does not leak further information and
<command>Touch.force</command> does not reveal how much pressure a user applied
to the surface. That is achieved by a direct
<ulink url="https://gitweb.torproject.org/tor-browser.git/commit/?h=tor-browser-52.5.2esr-7.0-2&amp;id=7d9701c2b6a203b1b7a556f614858588e3e5976e">
Firefox patch</ulink> which reports back <command>1</command> for the first two
properties and <command>0.0</command> for the two last ones.

      </para>
    </listitem>

    <listitem><command>Battery Status API</command>
      <para>

The Battery Status API provides access to information about the system's battery
charge level. From Firefox 52 on it is disabled for web content. Initially, it
was possible on Linux to get a double-precision floating point value for the
charge level, which means there was a large number of possible values making it
almost behave like an identifier allowing to track a user cross-origin. But
still after that got fixed (and on other platforms where the precision was just
two significant digits anyway) the risk for tracking users remained as combined
with the <command>chargingTime</command> and <command>dischargingTime</command>
the possible values <ulink url="https://senglehardt.com/papers/iwpe17_battery_status_case_study.pdf">
got estimated to be in the millons</ulink> under normal conditions. We avoid all
those possible issues with disabling the Battery Status API by setting
<command>dom.battery.enabled</command> to <command>false</command>.

      </para>
    </listitem>

    <listitem><command>System Uptime</command>
      <para>

It is possible to get the system uptime of a Tor Browser user by querying the
<command>Event.timestamp</command> property. We avoid this by setting <command>
dom.event.highrestimestamp.enabled</command> to <command>true</command>.

      </para>
    </listitem>

    <listitem><command>Keyboard Layout Fingerprinting</command>
      <para>

<command>KeyboardEvent</command>s provide a way for a website to find out
information about the keyboard layout of its visitors. In fact there are <ulink url="https://developers.google.com/web/updates/2016/04/keyboardevent-keys-codes">
several dimensions</ulink> to this fingerprinting vector. The <command>
KeyboardEvent.code</command> property represents a physical key that can't be
changed by the keyboard layout nor by the modifier state. On the other hand the
<command>KeyboardEvent.key</command> property contains the character that is
generated by that key. This is dependent on things like keyboard layout, locale
and modifier keys.

      </para>
      <para><command>Design Goal:</command>

Websites MUST NOT be able to infer any information about the keyboard of a Tor
Browser user.

      </para>
      <para><command>Implementation Status:</command>

We provide <ulink url="https://gitweb.torproject.org/tor-browser.git/commit/?h=tor-browser-52.5.2esr-7.0-2&amp;id=d6d29f155e60c63b38918c8879ee221b9c90b1f7">two</ulink>
<ulink url="https://gitweb.torproject.org/tor-browser.git/commit/?h=tor-browser-52.5.2esr-7.0-2&amp;id=789bad5fe5a7a0c2d27e1d8dd7b9a7e35de91cc8">Firefox patches</ulink>
that take care of spoofing <command>KeyboardEvent.code</command> and <command>
KeyboardEvent.keyCode</command> by providing consensus (US-English-style) fake
properties. This is achieved by hiding the user's use of the numpad, and any
non-QWERTY US English keyboard. Characters from non-en-US languages
are currently returning an empty <command>KeyboardEvent.code</command> and a
<command>KeyboardEvent.keyCode</command> of <command>0</command>. Moreover,
neither <command>Alt</command> or <command>Shift</command>, or
<command>AltGr</command> keyboard events are reported to content.
      </para>
    </listitem>
    <listitem><command>User Agent and HTTP Headers</command>
     <para><command>Design Goal:</command>

All Tor Browser users MUST provide websites with an identical user agent and
HTTP header set for a given request type. We omit the Firefox minor revision,
and report a popular Windows platform. If the software is kept up to date,
these headers should remain identical across the population even when updated.

     </para>
     <para><command>Implementation Status:</command>

Firefox provides several options for controlling the browser user agent string
which we leverage. We also set similar prefs for controlling the
Accept-Language and Accept-Charset headers, which we spoof to English by default. Additionally, we
<ulink
url="https://gitweb.torproject.org/tor-browser.git/commit/?h=tor-browser-52.5.2esr-7.0-2&amp;id=bd51d0c24d339c5135028297f5eeb591a65e99df">remove
content script access</ulink> to Components.interfaces, which <ulink
url="http://pseudo-flaw.net/tor/torbutton/fingerprint-firefox.html">can be
used</ulink> to fingerprint OS, platform, and Firefox minor version.  </para>

    </listitem>

    <listitem><command>Timing-based Side Channels</command>
      <para>
Attacks based on timing side channels are nothing new in the browser context.
<ulink url="http://sip.cs.princeton.edu/pub/webtiming.pdf">Cache-based</ulink>,
<ulink url="https://www.abortz.net/papers/timingweb.pdf">cross-site timing</ulink>,
and <ulink url="https://www.contextis.com/documents/2/Browser_Timing_Attacks.pdf">
pixel stealing</ulink>, to name just a few, got investigated in the past.
While their fingerprinting potential varies all timing-based attacks have in
common that they need sufficiently fine-grained clocks.
      </para>
      <para><command>Design Goal:</command>

Websites MUST NOT be able to fingerprint a Tor Browser user by exploiting
timing-based side channels.

      </para>
      <para><command>Implementation Status:</command>

The cleanest solution to timing-based side channels would be to get rid of them.
This has been <ulink url="https://acmccs.github.io/papers/p163-caoA.pdf">proposed</ulink>
in the research community. However, we remain skeptical as it does not seem to
be trivial even considering just a
<ulink url="https://bugzilla.mozilla.org/show_bug.cgi?id=711043">single</ulink>
<ulink url="https://cseweb.ucsd.edu/~dkohlbre/papers/subnormal.pdf">side channel</ulink>
and <ulink url="https://gruss.cc/files/fantastictimers.pdf">more and more
potential side channels</ulink> are showing up. Thus, we rely on disabling all
possible timing sources or making them coarse-grained enough in order to render
timing side channels unsuitable as a means for fingerprinting browser users.

      </para>

      <para>

We set <command>dom.enable_user_timing</command> and
<command>dom.enable_resource_timing</command> to <command>false</command> to
disable these explicit timing sources. Furthermore, we clamp the resolution of
explicit clocks to 100ms <ulink url="https://gitweb.torproject.org/tor-browser.git/commit/?h=tor-browser-52.5.2esr-7.0-2&amp;id=1736ea256276546c899d712dffdae2c8d050d8a0">with two Firefox</ulink> <ulink url="https://gitweb.torproject.org/tor-browser.git/commit/?h=tor-browser-52.5.2esr-7.0-2&amp;id=a4c6d2c07d483acfd729c7a50dd3f7b07fcba03a">patches</ulink>.

This includes <command>performance.now()</command>, <command>new Date().getTime()
</command>, <command>audioContext.currentTime</command>, <command>
canvasStream.currentTime</command>, <command>video.currentTime</command>,
<command>audio.currentTime</command>, <command>new File([], "").lastModified
</command>, <command>new File([], "").lastModifiedDate.getTime()</command>,
<command>animation.startTime</command>, <command>animation.currentTime</command>,
<command>animation.timeline.currentTime</command>,
and <command>document.timeline.currentTime</command>.

      </para>
      <para>

While clamping the clock resolution to 100ms is a step towards neutering the
timing-based side channel fingerprinting, it is by no means sufficient. It turns
out that it is possible to subvert our clamping of explicit clocks by using
<ulink url="https://www.usenix.org/system/files/conference/usenixsecurity16/sec16_paper_kohlbrenner.pdf">
implicit ones</ulink>, e.g. extrapolating the true time by running a busy loop
with a predictable operation in it. We are tracking
 <ulink url="https://trac.torproject.org/projects/tor/ticket/16110">this problem
</ulink> in our bug tracker and are working with the research community and
Mozilla to develop and test a proper solution to this part of our defense
against timing-based side channel fingerprinting risks.

      </para>
    </listitem>

    <listitem><command>resource:// and chrome:// URIs Leaks</command>
      <para>
Due to <ulink url="https://bugzilla.mozilla.org/show_bug.cgi?id=863246">bugs
</ulink> <ulink url="https://bugzilla.mozilla.org/show_bug.cgi?id=1120398">
in Firefox</ulink> it is possible to detect the locale and the platform of a
Tor Browser user. Moreover, it is possible to
<ulink url="https://www.usenix.org/system/files/conference/usenixsecurity17/sec17-sanchez-rola.pdf">
find out the extensions</ulink> a user has installed. This is done by
including resource:// and/or chrome:// URIs into web content, which point to
resources included in Tor Browser itself or in installed extensions, and
exploiting the different behavior resulting out of that: the browser raises
an exception if a webpage requests a resource but the extension is not
installed. This does not happen if the extension is indeed installed but the
resource path does not exist.
      </para>
      <para>

We believe that it should be impossible for web content to extract information
out of a Tor Browser user by deploying resource:// and/or chrome:// URIs. Until
this is fixed in Firefox <ulink url="https://gitweb.torproject.org/torbutton.git/tree/src/components/content-policy.js">
we filter</ulink> resource:// and chrome:// requests done
by web content denying them by default. We need a whitelist of resource:// and
chrome:// URIs, though, to avoid breaking parts of Firefox. Those more than a
dozen Firefox resources do not aid in fingerprinting Tor Browser users as they
are not different on the platforms and in the locales we support.

      </para>

    </listitem>
    <listitem><command>Locale Fingerprinting</command>
     <para>

In Tor Browser, we provide non-English users the option of concealing their OS
and browser locale from websites. It is debatable if this should be as high of
a priority as information specific to the user's computer, but for completeness,
we attempt to maintain this property.

     </para>
     <para><command>Implementation Status:</command>

We set the fallback character set to set to windows-1252 for all locales, via
<command>intl.charset.default</command>. We also set
<command>javascript.use_us_english_locale</command> to <command>true</command>
to instruct the JS engine to use en-US as its internal C locale for all Date,
Math, and exception handling. Additionally, we provide a patch to use an
<ulink url="https://gitweb.torproject.org/tor-browser.git/commit/?h=tor-browser-52.5.2esr-7.0-2&amp;id=d144738fedeeb23746d7a9f16067bd985b0d59aa">
en-US label for the <command>isindex</command> HTML element</ulink> instead of
letting the label leak the browser's UI locale.
     </para>
    </listitem>
    <listitem><command>Timezone and Clock Offset</command>
     <para>

While the latency in Tor connections varies anywhere from milliseconds to
a few seconds, it is still possible for the remote site to detect large
differences between the user's clock and an official reference time source.

     </para>

  <para><command>Design Goal:</command>

All Tor Browser users MUST report the same timezone to websites. Currently, we
choose UTC for this purpose, although an equally valid argument could be made
for EDT/EST due to the large English-speaking population density (coupled with
the fact that we spoof a US English user agent). Additionally, the Tor
software should detect if the user's clock is significantly divergent from the
clocks of the relays that it connects to, and use this to reset the clock
values used in Tor Browser to something reasonably accurate. Alternatively,
the browser can obtain this clock skew via a mechanism similar to that used in
<ulink url="https://github.com/ioerror/tlsdate">tlsdate</ulink>.

     </para>
     <para><command>Implementation Status:</command>

We <ulink url="https://gitweb.torproject.org/tor-browser.git/commit/?h=tor-browser-52.5.2esr-7.0-2&amp;id=dd1ba0b5c9281ee3207e5a87991159b8d2609a11">
set the timezone to UTC</ulink> with a Firefox patch using the TZ environment
variable, which is supported on all platforms. Moreover, with an additional
patch just needed for the Windows platform, <ulink url="https://gitweb.torproject.org/tor-browser.git/commit/?h=tor-browser-52.5.2esr-7.0-2&amp;id=008649e2ce0357f31eb67d874e6429c39ddd7e8f">
we make sure</ulink> the TZ environment variable is respected by the
<ulink url="http://site.icu-project.org/">ICU library</ulink> as well.

     </para>
    </listitem>
    <listitem><command>JavaScript Performance Fingerprinting</command>
     <para>

<ulink url="https://cseweb.ucsd.edu/~hovav/dist/jspriv.pdf">JavaScript
performance fingerprinting</ulink> is the act of profiling the performance of
various JavaScript functions for the purpose of fingerprinting the JavaScript
engine and the CPU.

     </para>
     <para><command>Design Goal:</command>

We have <ulink
url="https://trac.torproject.org/projects/tor/ticket/3059">several potential
mitigation approaches</ulink> to reduce the accuracy of performance
fingerprinting without risking too much damage to functionality. Our current
favorite is to reduce the resolution of the Event.timeStamp and the JavaScript
Date() object, while also introducing jitter. We believe that JavaScript time
resolution may be reduced all the way up to the second before it seriously
impacts site operation. Our goal with this quantization is to increase the
amount of time it takes to mount a successful attack. <ulink
url="https://cseweb.ucsd.edu/~hovav/dist/jspriv.pdf">Mowery et al</ulink> found
that even with the default precision in most browsers, they required up to 120
seconds of amortization and repeated trials to get stable results from their
feature set. We intend to work with the research community to establish the
optimum trade-off between quantization+jitter and amortization time, as well
as identify highly variable JavaScript operations. As long as these attacks
take several seconds or more to execute, they are unlikely to be appealing to
advertisers, and are also very likely to be noticed if deployed against a
large number of people.

     </para>
     <para><command>Implementation Status:</command>

Currently, our mitigation against performance fingerprinting is to
disable <ulink url="https://www.w3.org/TR/navigation-timing/">Navigation
Timing</ulink> by setting the Firefox preference
<command>dom.enable_performance</command> to <command>false</command>, and to
disable the <ulink
url="https://developer.mozilla.org/en-US/docs/Web/API/HTMLVideoElement#Gecko-specific_properties">Mozilla
Video Statistics</ulink> API extensions by setting the preference
<command>media.video_stats.enabled</command> to <command>false</command>, too.

     </para>
    </listitem>
    <listitem><command>Keystroke Fingerprinting</command>
     <para>

Keystroke fingerprinting is the act of measuring key strike time and key
flight time. It is seeing increasing use as a biometric.

     </para>
     <para><command>Design Goal:</command>

We intend to rely on the same mechanisms for defeating JavaScript performance
fingerprinting: timestamp quantization and jitter.

     </para>

     <para><command>Implementation Status:</command>

We clamp keyboard event resolution to 100ms with a <ulink url="https://gitweb.torproject.org/tor-browser.git/commit/?h=tor-browser-52.5.2esr-7.0-2&amp;id=1736ea256276546c899d712dffdae2c8d050d8a0">Firefox patch</ulink>.

     </para>
    </listitem>
    <listitem><command>Amount of Processor Cores (hardwareConcurrency)</command>
      <para>

Modern computers have multiple physical processor cores in their CPU available.
One core typically allows to run more than one thread at a time and
<command>navigator.hardwareConcurrency</command> makes the number of those
threads (i.e. logical processors) available to web content.

      </para>
      <para><command>Design Goal:</command>

Websites MUST NOT be able to fingerprint a Tor Browser user taking advantage of
the amount of logical processors available.

      </para>
      <para><command>Implementation Status:</command>

We set <command>dom.maxHardwareConcurrency</command> to <command>1</command> to
report the same amount of logical processors for everyone. However, there are
<ulink url="https://github.com/oftn/core-estimator">probablistic ways of
determining the same information available</ulink> which we are not defending
against currently. Moreover, we might even want to think about a more elaborate
approach defending against this fingerprinting technique by not making all users
uniform but rather <ulink url="https://bugs.torproject.org/22127">by following
a bucket approach</ulink> as we currently do in our defense against screen
size exfiltration.

      </para>
    </listitem>

    <listitem><command>Web Audio API</command>
      <para>

The <ulink url="https://developer.mozilla.org/en-US/docs/Web/API/Web_Audio_API">
Web Audio API</ulink> provides several means to aid in fingerprinting users.
At the simplest level it allows differentiating between users having the API
available and those who don't by checking for an <command>AudioContext</command>
or <command>OscillatorNode</command> object. However, there are more bits of
information that the Web Audio API reveals if audio signals generated with an
<command>OscillatorNode</command> are processed as
<ulink url="https://senglehardt.com/papers/ccs16_online_tracking.pdf">hardware
and software differences</ulink> influence those results.

      </para>
      <para>

We disable the Web Audio API by setting <command>dom.webaudio.enabled</command>
to <command>false</command>. That has the positive side effect that it disables
one of several means to perform
<ulink url="https://petsymposium.org/2017/papers/issue2/paper18-2017-2-source.pdf">
ultrasound cross-device tracking</ulink> as well, which is based on having
<command>AudioContext</command> available.

      </para>
    </listitem>

    <listitem><command>MediaError.message</command>
      <para>

The <command>MediaError</command> object allows the user agent to report errors
that occurred while handling media, for instance using <command>audio</command>
or <command>video</command> elements. The <command>message</command> property
provides specific diagnostic information to help understanding the error
condition. As a defense-in-depth we make sure that no information aiding in
fingerprinting is leaking to websites that way
<command url="https://gitweb.torproject.org/tor-browser.git/commit/?h=tor-browser-52.5.2esr-7.0-2&amp;id=ee404a8341b5ecef535bfc0cad89b1682598fb68">
by returning just an empty string</command>.

      </para>
    </listitem>
    <listitem><command>Connection State</command>
     <para>

It is possible to monitor the connection state of a browser over time with
<ulink url="https://developer.mozilla.org/en-US/docs/Web/API/NavigatorOnLine/onLine">
navigator.onLine</ulink>. We prevent this by setting <command>
network.manage-offline-status</command> to <command>false</command>.

     </para>
    </listitem>
    <listitem><command>Reader View</command>
     <para>

<ulink url="https://support.mozilla.org/t5/Basic-Browsing/Firefox-Reader-View-for-clutter-free-web-pages/ta-p/38466">Reader View</ulink>
is a Firefox feature to view web pages clutter-free and easily adjusted to
own needs and preferences. To avoid fingerprintability risks we make Tor Browser
users uniform by setting <command>reader.parse-on-load.enabled</command> to
<command>false</command> and <command>browser.reader.detectedFirstArticle</command>
to <command>true</command>.

     </para>
    </listitem>
    <listitem><command>Contacting Mozilla Services</command>
      <para>

Tor Browser is based on Firefox which is a Mozilla product. Quite naturally,
Mozilla is interested in making users aware of new features and in gathering
information to learn about the most pressing needs Firefox users are facing.
This is often implemented by contacting Mozilla services, be it for displaying
further information about a new feature or by
<ulink url="https://wiki.mozilla.org/Telemetry">sending (aggregated) data back
for analysis</ulink>. While some of those mechanisms are disabled by default on
release channels (gathering telemetry data comes to mind) others are not. We
make sure that non of those Mozilla services is contacted to avoid possible
fingerprinting risks.

      </para>
      <para>

In particular, we disable GeoIP-based search results by setting <command>
browser.search.countryCode</command> and <command>browser.search.region
</command> to <command>US</command> and <command>browser.search.geoip.url
</command> to the empty string. Furthermore, we disable Selfsupport and Unified
Telemetry by setting <command>browser.selfsupport.enabled</command> and <command>
toolkit.telemetry.unified</command> to <command>false</command> and we make
sure no related ping is reaching Mozilla by setting <command>
datareporting.healthreport.about.reportUrlUnified</command> to <command>
data:text/plain,</command>. The same is done with <command>
datareporting.healthreport.about.reportUrl</command> and the new tiles feature
related <command>browser.newtabpage.directory.ping</command> and <command>
browser.newtabpage.directory.source</command> preferences.
<command>browser.newtabpage.remote</command> is set to <command>false</command>
in this context as well, as a defense-in-depth given that this feature is
already of by default. Additionally, we disable the UITour backend by setting
<command>browser.uitour.enabled</command> to <command>false</command> and avoid
getting Mozilla experiments installed into Tor Browser by flipping
<command>experiments.enabled</command> to <command>false</command>. On the
update side we prevent the browser from pinging the new
<ulink url="https://wiki.mozilla.org/Firefox/Kinto">Kinto</ulink> service for
blocklist updates as it is not used for it yet anyway. This is done by setting
<command>services.blocklist.update_enabled</command> to <command>false</command>.
The captive portal detection code is disabled as well as it phones home to
Mozilla. We set <command>network.captive-portal-service.enabled</command> to
<command>false</command> to achieve that. Unrelated to that we make sure that
Mozilla does not get bothered with TLS error reports from Tor Browser users by
hiding the respective checkbox with
<command>security.ssl.errorReporting.enabled</command> set to
<command>false</command>. And while we have the Push API disabled as there are
no Service Workers available in Tor Browser yet, we remove the value for
<command>dom.push.serverURL</command> as a defense-in-depth. Finally, we provide
<ulink url="https://gitweb.torproject.org/tor-browser.git/commit/?h=tor-browser-52.5.2esr-7.0-2&amp;id=9f24ce35cd8776a0f7c3a4d54992ecb0eaad6311">a patch</ulink>
to prevent Mozilla's websites from querying whether particular extensions are
installed and what their state in Tor Browser is by using the
<command>window.navigator.AddonManager</command> API. As a defense-in-depth the
patch makes sure that not only Mozilla's websites can't get at that information
but that the whitelist governing this access is empty in general.

      </para>

      <para>

We have <ulink url="https://wiki.mozilla.org/Security/Safe_Browsing">Safebrowsing</ulink>
disabled in Tor Browser. In order to avoid pinging providers for list updates we
remove the entries for <command>browser.safebrowsing.provider.mozilla.updateURL</command>
and <command>browser.safebrowsing.provider.mozilla.gethashURL</command> (and the
values for Google related preferences as well).

      </para>
    </listitem>
    <listitem><command>Operating System Type Fingerprinting</command>
     <para>

As we mentioned in the introduction of this section, OS type fingerprinting is
currently considered a lower priority, due simply to the numerous ways that
characteristics of the operating system type may leak into content, and the
comparatively low contribution of OS to overall entropy. In particular, there
are likely to be many ways to measure the differences in widget size,
scrollbar size, and other rendered details on a page. Also, directly exported
OS routines (such as those from the standard C math library) expose
differences in their implementations through their return values.

     </para>
     <para><command>Design Goal:</command>

We intend to reduce or eliminate OS type fingerprinting to the best extent
possible, but recognize that the effort for reward on this item is not as high
as other areas. The entropy on the current OS distribution is somewhere around
2 bits, which is much lower than other vectors which can also be used to
fingerprint configuration and user-specific information. You can see the
major areas of OS fingerprinting we're aware of using the <ulink
url="https://trac.torproject.org/projects/tor/query?keywords=~tbb-fingerprinting-os">tbb-fingerprinting-os
tag on our bug tracker</ulink>.

     </para>
     <para><command>Implementation Status:</command>

At least two HTML5 features have a different implementation status across the
major OS vendors and/or the underlying hardware: the <ulink
url="https://developer.mozilla.org/en-US/docs/DOM/window.navigator.connection">Network
Connection API</ulink>, and the <ulink
url="https://wiki.mozilla.org/Sensor_API">Sensor API</ulink>. We disable these APIs through the Firefox preferences
<command>dom.network.enabled</command> and
<command>device.sensors.enabled</command>, setting both to <command>false</command>.

     </para>
    </listitem>
   </orderedlist>
   <para>
For more details on fingerprinting bugs and enhancements, see the <ulink
url="https://trac.torproject.org/projects/tor/query?keywords=~tbb-fingerprinting&amp;status=!closed">tbb-fingerprinting tag in our bug tracker</ulink>
   </para>
   </sect3>

<!--
   <sect3 id="fingerprinting-evaluation">
    <title>Studying the Efficacy of Fingerprinting Defenses</title>
     <para>

TODO: Describe what an ideal implementation of Panopticlick would look like.

     </para>
   </sect3>
-->
  </sect2>
  <sect2 id="new-identity">
   <title>Long-Term Unlinkability via "New Identity" button</title>
   <para>

In order to avoid long-term linkability, we provide a "New Identity" context
menu option in Torbutton. This context menu option is active if Torbutton can
read the environment variables $TOR_CONTROL_PASSWD and $TOR_CONTROL_PORT.

   </para>

   <sect3>
    <title>Design Goal:</title>
    <blockquote>

All linkable identifiers and browser state MUST be cleared by this feature.

    </blockquote>
   </sect3>

   <sect3>
    <title>Implementation Status:</title>
   <blockquote>
     <para>

First, Torbutton disables JavaScript in all open tabs and windows by using
both the <ulink
url="https://developer.mozilla.org/en-US/docs/XPCOM_Interface_Reference/nsIDocShell#Attributes">browser.docShell.allowJavaScript</ulink>
attribute as well as <ulink
url="https://developer.mozilla.org/en-US/docs/XPCOM_Interface_Reference/nsIDOMWindowUtils#suppressEventHandling%28%29">nsIDOMWindowUtil.suppressEventHandling()</ulink>.
We then stop all page activity for each tab using <ulink
url="https://developer.mozilla.org/en-US/docs/XPCOM_Interface_Reference/nsIWebNavigation#stop%28%29">browser.webNavigation.stop(nsIWebNavigation.STOP_ALL)</ulink>.
We then clear the site-specific Zoom by temporarily disabling the preference
<command>browser.zoom.siteSpecific</command>, and clear the GeoIP wifi token URL
<command>geo.wifi.access_token</command> and the last opened URL preference (if
it exists). Each tab is then closed.

     </para>
     <para>

After closing all tabs, we then clear the searchbox and findbox text and emit
"<ulink url="https://developer.mozilla.org/en-US/docs/Supporting_private_browsing_mode#Private_browsing_notifications">browser:purge-session-history</ulink>"
(which instructs addons and various Firefox components to clear their session
state). Then we manually clear the following state: HTTP auth, SSL state,
crypto tokens, OCSP state, site-specific content preferences (including HSTS
state), the undo tab history, content and image cache, offline and memory cache,
offline storage, IndexedDB storage, asm.js cache, cookies, DOM storage, the
safe browsing key, the Google wifi geolocation token (if it exists), and the
domain isolator state. We also clear NoScript's site and temporary permissions,
and all other browser site permissions.

     </para>
     <para>

After the state is cleared, we then close all remaining HTTP Keep-Alive
connections and then send the NEWNYM signal to the Tor control port to cause a
new circuit to be created.
     </para>
     <para>

Finally, a fresh browser window is opened, and the current browser window is
closed (this does not spawn a new Firefox process, only a new window). Upon
the close of the final window, an unload handler is fired to invoke the <ulink
url="https://developer.mozilla.org/en-US/docs/Mozilla/Tech/XPCOM/Reference/Interface/nsIDOMWindowUtils#garbageCollect%28%29">garbage
collector</ulink>, which has the effect of immediately purging any blob:UUID
URLs that were created by website content via <ulink
url="https://developer.mozilla.org/en-US/docs/Web/API/URL/createObjectURL">URL.createObjectURL</ulink>.

     </para>
    </blockquote>
<!--    <blockquote>
If the user chose to "protect" any cookies by using the Torbutton Cookie
Protections UI, those cookies are not cleared as part of the above.
    </blockquote>
-->
   </sect3>
  </sect2>
<!--
  <sect2 id="click-to-play">
   <title>Click-to-play for plugins and invasive content</title>
   <para>
Some content types are too invasive and/or too opaque for us to properly
eliminate their linkability properties. For these content types, we use
NoScript to provide click-to-play placeholders that do not activate the
content until the user clicks on it. This will eliminate the ability for an
adversary to use such content types to link users in a dragnet fashion across
arbitrary sites.
   </para>
   <para>
Currently, the content types isolated in this way include Flash, WebGL, and
audio and video objects.
   </para>
  </sect2>
-->
  <sect2 id="other-security">
   <title>Other Security Measures</title>
   <para>

In addition to the above mechanisms that are devoted to preserving privacy
while browsing, we also have a number of technical mechanisms to address other
privacy and security issues.

   </para>
   <orderedlist>
    <listitem id="security-slider"><command>Security Slider</command>
     <para>
In order to provide vulnerability surface reduction for users that need high
security, we have implemented a "Security Slider" to allow users to make a
tradeoff between usability and security while minimizing the total number of
choices (to reduce fingerprinting). Using metrics collected from
Mozilla's bug tracker, we analyzed the vulnerability counts of core
components, and used <ulink
url="https://github.com/iSECPartners/publications/tree/master/reports/Tor%20Browser%20Bundle">information
gathered from a study performed by iSec Partners</ulink> to inform which
features should be disabled at which security levels.

     </para>
     <para>

The Security Slider consists of three positions:

     </para>
     <itemizedlist>
      <listitem><command>Low (default)</command>
      <para>

At this security level, the preferences are the Tor Browser defaults. This
includes three features that were formerly governed by the slider at
higher security levels: <command>gfx.font_rendering.graphite.enabled</command>
is set to <command>false</command> now after Mozilla got convinced that
<ulink url="https://bugzilla.mozilla.org/show_bug.cgi?id=1255731">leaving
it enabled is too risky</ulink>. Even though Mozilla reverted that decision
after another round of fixing critical Graphite bugs, we remain skeptical
and keep that feature disabled for now. <command>network.jar.block-remote-files</command>
is set to <command>true</command>. Mozilla tried to block remote JAR files in
Firefox 45 but needed to revert that decision due to breaking IBM's iNotes.
While Mozilla <ulink url="https://bugzilla.mozilla.org/show_bug.cgi?id=1329336">
is working on getting this disabled again</ulink> we take the protective stance
already now and block remote JAR files even on the low security level. Finally,
we exempt asm.js from the security slider and block it on all levels. See the
<link linkend="disk-avoidance">Disk Avoidance</link> and the cache linkability
concerns in the <link linkend="identifier-linkability">Cross-Origin Identifier
Unlinkability</link> sections for further details.

      </para>
      </listitem>
      <listitem><command>Medium</command>
       <para>

At this security level, we disable the ION JIT
(<command>javascript.options.ion</command>), native regular expressions
(<command>javascript.options.native_regexp</command>), Baseline JIT
(<command>javascript.options.baselinejit</command>), WebAudio
(<command>media.webaudio.enabled</command>), MathML
(<command>mathml.disabled</command>), SVG Opentype font rendering
(<command>gfx.font_rendering.opentype_svg.enabled</command>), and make HTML5 audio
and video click-to-play via NoScript (<command>noscript.forbidMedia</command>).
Furthermore, we only allow JavaScript to run if it is loaded over HTTPS and the
URL bar is HTTPS (by setting <command>noscript.global</command> to false and
<command>noscript.globalHttpsWhitelist</command> to true).

       </para>
      </listitem>
      <listitem><command>High</command>
       <para>

This security level inherits the preferences from the Medium level, and
additionally disables remote fonts (<command>noscript.forbidFonts</command>),
completely disables JavaScript (by
unsetting <command>noscript.globalHttpsWhitelist</command>), and disables SVG
images (<command>svg.in-content.enabled</command>).

       </para>
      </listitem>
     </itemizedlist>
    </listitem>
    <listitem id="traffic-fingerprinting-defenses"><command>Website Traffic Fingerprinting Defenses</command>
     <para>

<link linkend="website-traffic-fingerprinting">Website Traffic
Fingerprinting</link> is a statistical attack to attempt to recognize specific
encrypted website activity.

     </para>
     <sect3>
       <title>Design Goal:</title>
       <blockquote>
      <para>

We want to deploy a mechanism that reduces the accuracy of <ulink
url="https://en.wikipedia.org/wiki/Feature_selection">useful features</ulink> available
for classification. This mechanism would either impact the true and false
positive accuracy rates, <emphasis>or</emphasis> reduce the number of web pages
that could be classified at a given accuracy rate.

     </para>
     <para>

Ideally, this mechanism would be as light-weight as possible, and would be
tunable in terms of overhead. We suspect that it may even be possible to
deploy a mechanism that reduces feature extraction resolution without any
network overhead. In the no-overhead category, we have <ulink
url="https://freehaven.net/anonbib/cache/LZCLCP_NDSS11.pdf">HTTPOS</ulink> and
<ulink
url="https://blog.torproject.org/blog/experimental-defense-website-traffic-fingerprinting">better
use of HTTP pipelining and/or SPDY</ulink>.
In the tunable/low-overhead
category, we have <ulink
url="https://arxiv.org/abs/1512.00524">Adaptive
Padding</ulink> and <ulink url="https://www3.cs.stonybrook.edu/~xcai/fp.pdf">
Congestion-Sensitive BUFLO</ulink>. It may be also possible to <ulink
url="https://trac.torproject.org/projects/tor/ticket/7028">tune such
defenses</ulink> such that they only use existing spare Guard bandwidth capacity in the Tor
network, making them also effectively no-overhead.

     </para>
       </blockquote>
     </sect3>
     <sect3>
       <title>Implementation Status:</title>
       <blockquote>
       <para>
Currently, we patch Firefox to <ulink
url="https://gitweb.torproject.org/tor-browser.git/commit/?h=tor-browser-52.5.2esr-7.0-2&amp;id=b9fa77472aa67e26bd46a5ca889b20ce3448f9d1">randomize
pipeline order and depth</ulink>. Unfortunately, pipelining is very fragile.
Many sites do not support it, and even sites that advertise support for
pipelining may simply return error codes for successive requests, effectively
forcing the browser into non-pipelined behavior. Firefox also has code to back
off and reduce or eliminate the pipeline if this happens. These
shortcomings and fallback behaviors are the primary reason that Google
developed SPDY as opposed to simply extending HTTP to improve pipelining. It
turns out that we could actually deploy exit-side proxies that allow us to
<ulink
url="https://gitweb.torproject.org/torspec.git/tree/proposals/ideas/xxx-using-spdy.txt">use
SPDY from the client to the exit node</ulink>. This would make our defense not
only free, but one that actually <emphasis>improves</emphasis> performance.

     </para>
     <para>

Knowing this, we created this defense as an <ulink
url="https://blog.torproject.org/blog/experimental-defense-website-traffic-fingerprinting">experimental
research prototype</ulink> to help evaluate what could be done in the best
case with full server support. Unfortunately, the bias in favor of compelling
attack papers has caused academia to ignore this request thus far, instead
publishing only cursory (yet "devastating") evaluations that fail to provide
even simple statistics such as the rates of actual pipeline utilization during
their evaluations, in addition to the other shortcomings and shortcuts <link
linkend="website-traffic-fingerprinting">mentioned earlier</link>. We can
accept that our defense might fail to work as well as others (in fact we
expect it), but unfortunately the very same shortcuts that provide excellent
attack results also allow the conclusion that all defenses are broken forever.
So sadly, we are still left in the dark on this point.

     </para>
      </blockquote>
    </sect3>
    </listitem>
    <listitem><command>Privacy-preserving update notification</command>
     <para>

In order to inform the user when their Tor Browser is out of date, we perform a
privacy-preserving update check asynchronously in the background. The
check uses Tor to download the file <ulink
url="https://check.torproject.org/RecommendedTBBVersions">https://check.torproject.org/RecommendedTBBVersions</ulink>
and searches that version list for the current value for the local preference
<command>torbrowser.version</command>. If the value from our preference is
present in the recommended version list, the check is considered to have
succeeded and the user is up to date. If not, it is considered to have failed
and an update is needed. The check is triggered upon browser launch, new
window, and new tab, but is rate limited so as to happen no more frequently
than once every 1.5 hours.

     </para>
     <para>

If the check fails, we cache this fact, and update the Torbutton graphic to
display a flashing warning icon and insert a menu option that provides a link
to our download page. Additionally, we reset the value for the browser
homepage to point to a <ulink
url="https://check.torproject.org/?lang=en-US&amp;small=1&amp;uptodate=0">page that
informs the user</ulink> that their browser is out of
date.

     </para>
     <para>

We also make use of the in-browser Mozilla updater, and have <ulink
url="https://gitweb.torproject.org/tor-browser.git/commit/?h=tor-browser-52.5.2esr-7.0-2&amp;id=0efd496826cc3dfb0a6874d150e8acecd4eb6a92">patched
the updater</ulink> to avoid sending OS and Kernel version information as part
of its update pings.

     </para>
    </listitem>

   </orderedlist>
  </sect2>
</sect1>

<!--
- Packaging
  - Build Process Security
  - External Addons
    - Included
      - HTTPS-E
      - NoScript
      - Torbutton
    - Deliberately excluded
      - Request Policy, AdblockPlus, etc
    - Desired
      - Perspectives/Convergence/etc
  - Pref Changes
    - Caused by Torbutton
    - Set manually in profile
  - Update security
    - Thandy

-->

<sect1 id="BuildSecurity">
  <title>Build Security and Package Integrity</title>
  <para>

In the age of state-sponsored malware, <ulink
url="https://blog.torproject.org/blog/deterministic-builds-part-one-cyberwar-and-global-compromise">we
believe</ulink> it is impossible to expect to keep a single build machine or
software signing key secure, given the class of adversaries that Tor has to
contend with. For this reason, we have deployed a build system
that allows anyone to use our source code to reproduce byte-for-byte identical
binary packages to the ones that we distribute.

  </para>

  <sect2>
   <title>Achieving Binary Reproducibility</title>
   <para>

The GNU toolchain has been working on providing reproducible builds for some
time, however a large software project such as Firefox typically ends up
embedding a large number of details about the machine it was built on, both
intentionally and inadvertently. Additionally, manual changes to the build
machine configuration can accumulate over time and are difficult for others to
replicate externally, which leads to difficulties with binary reproducibility.

   </para>

   <para>
For this reason, we decided to leverage the work done by the <ulink
url="https://gitian.org/">Gitian Project</ulink> from the Bitcoin community.
Gitian is a wrapper around Ubuntu's virtualization tools that allows you to
specify an Ubuntu or Debian version, architecture, a set of additional packages,
a set of input files, and a bash build scriptlet in an YAML document called a
"Gitian Descriptor". This document is used to install a qemu-kvm image, and
execute your build scriptlet inside it.
   </para>

   <para>

We have created a <ulink
url="https://gitweb.torproject.org/builders/tor-browser-bundle.git/tree/refs/heads/master">set
of wrapper scripts</ulink> around Gitian to automate dependency download and
authentication, as well as transfer intermediate build outputs between the
stages of the build process. Because Gitian creates a Linux build environment,
we must use cross-compilation to create packages for Windows and macOS. For
Windows, we use mingw-w64 as our cross compiler. For macOS, we use cctools and
clang and a binary redistribution of the Mac OS 10.7 SDK.

   </para>

   <para>

The use of the Gitian system eliminates build non-determinism by normalizing
the build environment's hostname, username, build path, uname output,
toolchain versions, and time. On top of what Gitian provides, we also had to
address the following additional sources of non-determinism:

   </para>

   <orderedlist>
   <listitem><command>Filesystem and archive reordering</command>
    <para>

The most prevalent source of non-determinism in the components of Tor Browser
by far was various ways that archives (such as zip, tar, jar/ja, DMG, and
Firefox manifest lists) could be reordered. Many file archivers walk the
file system in inode structure order by default, which will result in ordering
differences between two different archive invocations, especially on machines
of different disk and hardware configurations.

    </para>
    <para>

The fix for this is to perform an additional sorting step on the input list
for archives, but care must be taken to instruct libc and other sorting routines
to use a fixed locale to determine lexicographic ordering, or machines with
different locale settings will produce different sort results. We chose the
'C' locale for this purpose. We created wrapper scripts for <ulink
url="https://gitweb.torproject.org/builders/tor-browser-bundle.git/tree/gitian/build-helpers/dtar.sh">tar</ulink>,
<ulink
url="https://gitweb.torproject.org/builders/tor-browser-bundle.git/tree/gitian/build-helpers/dzip.sh">zip</ulink>,
and <ulink
url="https://gitweb.torproject.org/builders/tor-browser-bundle.git/tree/gitian/build-helpers/ddmg.sh">DMG</ulink>
to aid in reproducible archive creation.

    </para>
   </listitem>

   <listitem><command>Uninitialized memory in toolchain/archivers</command>
    <para>

We ran into difficulties with both binutils and the DMG archive script using
uninitialized memory in certain data structures that ended up written to disk.
Our binutils fixes were merged upstream, but the DMG archive fix remains an
<ulink
url="https://gitweb.torproject.org/builders/tor-browser-bundle.git/tree/gitian/patches/libdmg.patch">independent
patch</ulink>.

    </para>
   </listitem>
   <listitem><command>Fine-grained timestamps and timezone leaks</command>
    <para>

The standard way of controlling timestamps in Gitian is to use libfaketime,
which hooks time-related library calls to provide a fixed timestamp. However,
due to our use of wine to run py2exe for python-based pluggable transports,
pyc timestamps had to be addressed with an additional <ulink
url="https://gitweb.torproject.org/builders/tor-browser-bundle.git/tree/gitian/build-helpers/pyc-timestamp.sh">helper
script</ulink>. The timezone leaks were addressed by setting the
<command>TZ</command> environment variable to UTC in our descriptors.

    </para>
   </listitem>
   <listitem><command>Deliberately generated entropy</command>
    <para>

In two circumstances, deliberately generated entropy was introduced in various
components of the build process. First, the BuildID Debuginfo identifier
(which associates detached debug files with their corresponding stripped
executables) was introducing entropy from some unknown source. We removed this
header using objcopy invocations in our build scriptlets, and opted to use GNU
DebugLink instead of BuildID for this association.

    </para>
    <para>

Second, on Linux, Firefox builds detached signatures of its cryptographic
libraries using a temporary key for FIPS-140 certification. A rather insane
subsection of the FIPS-140 certification standard requires that you distribute
signatures for all of your cryptographic libraries. The Firefox build process
meets this requirement by generating a temporary key, using it to sign the
libraries, and discarding the private portion of that key. Because there are
many other ways to intercept the crypto outside of modifying the actual DLL
images, we opted to simply remove these signature files from distribution.
There simply is no way to verify code integrity on a running system without
both OS and co-processor assistance. Download package signatures make sense of
course, but we handle those another way (as mentioned above).


    </para>
  </listitem>
  <listitem><command>LXC-specific leaks</command>
   <para>

Gitian provides an option to use LXC containers instead of full qemu-kvm
virtualization. Unfortunately, these containers can allow additional details
about the host OS to leak. In particular, umask settings as well as the
hostname and Linux kernel version can leak from the host OS into the LXC
container. We addressed umask by setting it explicitly in our Gitian
descriptor scriptlet, and addressed the hostname and kernel version leaks by
directly patching the aspects of the Firefox build process that included this
information into the build. It also turns out that some libraries (in
particular: libgmp) attempt to detect the current CPU to determine which
optimizations to compile in. This CPU type is uniform on our KVM instances,
but differs under LXC.

   </para>
  </listitem>
  </orderedlist>
  </sect2>

  <sect2>
    <title>Package Signatures and Verification</title>
    <para>

The build process generates a single sha256sums-unsigned-build.txt file that
contains a sorted list of the SHA-256 hashes of every package produced for that
build version. Each official builder uploads this file and a GPG signature of it
to a directory on a Tor Project's web server. The build scripts have an optional
matching step that downloads these signatures, verifies them, and ensures that
the local builds match this file.

    </para>
    <para>

When builds are published officially, the single sha256sums-unsigned-build.txt
file is accompanied by a detached GPG signature from each official builder that
produced a matching build. The packages are additionally signed with detached
GPG signatures from an official signing key.

    </para>
     <para>

The fact that the entire set of packages for a given version can be
authenticated by a single hash of the sha256sums-unsigned-build.txt file will
also allow us to create a number of auxiliary authentication mechanisms for our
packages, beyond just trusting a single offline build machine and a single
cryptographic key's integrity. Interesting examples include providing multiple
independent cryptographic signatures for packages, listing the package hashes in
the Tor consensus, and encoding the package hashes in the Bitcoin blockchain.

     </para>
    <para>

The Windows releases are also signed by a hardware token provided by Digicert.
In order to verify package integrity, the signature must be stripped off using
the osslsigncode tool, as described on the <ulink
url="https://www.torproject.org/docs/verifying-signatures.html.en#BuildVerification">Signature
Verification</ulink> page.

    </para>
  </sect2>

  <sect2>
    <title>Anonymous Verification</title>
    <para>

Due to the fact that bit-identical packages can be produced by anyone, the
security of this build system extends beyond the security of the official
build machines. In fact, it is still possible for build integrity to be
achieved even if all official build machines are compromised.

    </para>
    <para>

By default, all tor-specific dependencies and inputs to the build process are
downloaded over Tor, which allows build verifiers to remain anonymous and
hidden. Because of this, any individual can use our anonymity network to
privately download our source code, verify it against public, signed, audited,
and mirrored git repositories, and reproduce our builds exactly, without being
subject to targeted attacks. If they notice any differences, they can alert
the public builders/signers, hopefully using a pseudonym or our anonymous
bug tracker account, to avoid revealing the fact that they are a build
verifier.

   </para>
  </sect2>
  <sect2 id="update-safety">
   <title>Update Safety</title>
   <para>

We make use of the Firefox updater in order to provide automatic updates to
users. We make use of certificate pinning to ensure that update checks cannot
be tampered with by setting <command>security.cert_pinning.enforcement_level
</command> to <command>2</command>, and we sign the individual MAR update files
with keys that get rotated every year.

   </para>
   <para>

The Firefox updater also has code to ensure that it can reliably access the
update server to prevent availability attacks, and complains to the user after 48
hours go by without a successful response from the server. Additionally, we
use Tor's SOCKS username and password isolation to ensure that every new
request to the updater (provided the former got issued more than 10 minutes ago)
traverses a separate circuit, to avoid holdback attacks by exit nodes.

   </para>
  </sect2>


</sect1>
<!--
  <sect2 id="components">
   <title>Components</title>
   <para> </para>
   <sect3>
    <title>Included Addons</title>
   </sect3>
   <sect3>
    <title>Pluggable Transports</title>
   </sect3>
  </sect2>
  <sect2 id="prefs">
   <title>Pref Changes</title>
   <para> </para>
  </sect2>
  <sect2 id="update-mechanism">
   <title>Update Security</title>
   <para> </para>
  </sect2>
</sect1>


<sect1 id="Testing">
  <title>Testing</title>
  <para>

The purpose of this section is to cover all the known ways that Tor browser
security can be subverted from a penetration testing perspective. The hope
is that it will be useful both for creating a &quot;Tor Safety Check&quot;
page, and for developing novel tests and actively attacking Torbutton with the
goal of finding vulnerabilities in either it or the Mozilla components,
interfaces and settings upon which it relies.

  </para>
  <sect2 id="SingleStateTesting">
   <title>Single state testing</title>
   <para>

Torbutton is a complicated piece of software. During development, changes to
one component can affect a whole slough of unrelated features.  A number of
aggregated test suites exist that can be used to test for regressions in
Torbutton and to help aid in the development of Torbutton-like addons and
other privacy modifications of other browsers. Some of these test suites exist
as a single automated page, while others are a series of pages you must visit
individually. They are provided here for reference and future regression
testing, and also in the hope that some brave soul will one day decide to
combine them into a comprehensive automated test suite.

     <orderedlist>
      <listitem><ulink url="http://decloak.net/">Decloak.net</ulink>
       <para>

Decloak.net is the canonical source of plugin and external-application based
proxy-bypass exploits. It is a fully automated test suite maintained by <ulink
url="http://digitaloffense.net/">HD Moore</ulink> as a service for people to
use to test their anonymity systems.

       </para>
      </listitem>
      <listitem><ulink url="http://deanonymizer.com/">Deanonymizer.com</ulink>
       <para>

Deanonymizer.com is another automated test suite that tests for proxy bypass
and other information disclosure vulnerabilities. It is maintained by Kyle
Williams, the author of <ulink url="http://www.janusvm.com/">JanusVM</ulink>
and <ulink url="http://www.januspa.com/">JanusPA</ulink>.

       </para>
      </listitem>
      <listitem><ulink url="https://ip-check.info">JonDos
AnonTest</ulink>
       <para>

The <ulink url="https://anonymous-proxy-servers.net/">JonDos people</ulink> also provide an
anonymity tester. It is more focused on HTTP headers and behaviors than plugin bypass, and
points out a couple of headers Torbutton could do a better job with
obfuscating.

       </para>
      </listitem>
      <listitem><ulink url="http://browserspy.dk">Browserspy.dk</ulink>
       <para>

Browserspy.dk provides a tremendous collection of browser fingerprinting and
general privacy tests. Unfortunately they are only available one page at a
time, and there is not really solid feedback on good vs bad behavior in
the test results.

       </para>
      </listitem>
      <listitem><ulink url="http://analyze.privacy.net/">Privacy
Analyzer</ulink>
       <para>

The Privacy Analyzer provides a dump of all sorts of browser attributes and
settings that it detects, including some information on your original IP
address. Its page layout and lack of good vs bad test result feedback makes it
not as useful as a user-facing testing tool, but it does provide some
interesting checks in a single page.

       </para>
      </listitem>
      <listitem><ulink url="http://ha.ckers.org/mr-t/">Mr. T</ulink>
       <para>

Mr. T is a collection of browser fingerprinting and deanonymization exploits
discovered by the <ulink url="http://ha.ckers.org">ha.ckers.org</ulink> crew
and others. It is also not as user friendly as some of the above tests, but it
is a useful collection.

       </para>
      </listitem>
      <listitem>Gregory Fleischer's <ulink
url="http://pseudo-flaw.net/content/tor/torbutton/">Torbutton</ulink> and
<ulink
url="http://pseudo-flaw.net/content/defcon/dc-17-demos/d.html">Defcon
17</ulink> Test Cases
       <para>

Gregory Fleischer has been hacking and testing Firefox and Torbutton privacy
issues for the past 2 years. He has an excellent collection of all his test
cases that can be used for regression testing. In his Defcon work, he
demonstrates ways to infer Firefox version based on arcane browser properties.
We are still trying to determine the best way to address some of those test
cases.

       </para>
      </listitem>
      <listitem><ulink url="https://torcheck.xenobite.eu/index.php">Xenobite's
TorCheck Page</ulink>
       <para>

This page checks to ensure you are using a valid Tor exit node and checks for
some basic browser properties related to privacy. It is not very fine-grained
or complete, but it is automated and could be turned into something useful
with a bit of work.

       </para>
      </listitem>
     </orderedlist>
    </para>
  </sect2>
-->
<!--
  <sect2>
   <title>Multi-state testing</title>
   <para>

The tests in this section are geared towards a page that would instruct the
user to toggle their Tor state after the fetch and perform some operations:
mouseovers, stray clicks, and potentially reloads.

   </para>
   <sect3>
    <title>Cookies and Cache Correlation</title>
    <para>
The most obvious test is to set a cookie, ask the user to toggle tor, and then
have them reload the page. The cookie should no longer be set if they are
using the default Torbutton settings. In addition, it is possible to leverage
the cache to <ulink
url="http://crypto.stanford.edu/sameorigin/safecachetest.html">store unique
identifiers</ulink>. The default settings of Torbutton should also protect
against these from persisting across Tor Toggle.

    </para>
   </sect3>
   <sect3>
    <title>JavaScript timers and event handlers</title>
    <para>

JavaScript can set timers and register event handlers in the hopes of fetching
URLs after the user has toggled Torbutton.
    </para>
   </sect3>
   <sect3>
    <title>CSS Popups and non-script Dynamic Content</title>
    <para>

Even if JavaScript is disabled, CSS is still able to
<ulink url="http://www.tjkdesign.com/articles/css%20pop%20ups/">create popup-like
windows</ulink>
via the 'onmouseover' CSS attribute, which can cause arbitrary browser
activity as soon as the mouse enters into the content window. It is also
possible for meta-refresh tags to set timers long enough to make it likely
that the user has toggled Tor before fetching content.

    </para>
   </sect3>
  </sect2>
  <sect2 id="HackTorbutton">
   <title>Active testing (aka How to Hack Torbutton)</title>
   <para>

The idea behind active testing is to discover vulnerabilities in Torbutton to
bypass proxy settings, run script in an opposite Tor state, store unique
identifiers, leak location information, or otherwise violate <link
linkend="requirements">its requirements</link>. Torbutton has ventured out
into a strange and new security landscape. It depends on Firefox mechanisms
that haven't necessarily been audited for security, certainly not for the
threat model that Torbutton seeks to address. As such, it and the interfaces
it depends upon still need a 'trial by fire' typical of new technologies. This
section of the document was written with the intention of making that period
as fast as possible. Please help us get through this period by considering
these attacks, playing with them, and reporting what you find (and potentially
submitting the test cases back to be run in the standard batch of Torbutton
tests.

   </para>
   <sect3>
    <title>Some suggested vectors to investigate</title>
    <para>
    <itemizedlist>
     <listitem>Strange ways to register JavaScript <ulink
url="http://en.wikipedia.org/wiki/DOM_Events">events</ulink> and <ulink
url="http://www.devshed.com/c/a/JavaScript/Using-Timers-in-JavaScript/">timeouts</ulink> should
be verified to actually be ineffective after Tor has been toggled.</listitem>
     <listitem>Other ways to cause JavaScript to be executed after
<command>javascript.enabled</command> has been toggled off.</listitem>
     <listitem>Odd ways to attempt to load plugins. Kyle Williams has had
some success with direct loads/meta-refreshes of plugin-handled URLs.</listitem>
     <listitem>The Date and Timezone hooks should be verified to work with
crazy combinations of iframes, nested iframes, iframes in frames, frames in
iframes, and popups being loaded and
reloaded in rapid succession, and/or from one another. Think race conditions and deep,
parallel nesting, involving iframes from both <ulink
url="http://en.wikipedia.org/wiki/Same_origin_policy">same-origin and
non-same-origin</ulink> domains.</listitem>
     <listitem>In addition, there may be alternate ways and other
methods to query the timezone, or otherwise use some of the Date object's
methods in combination to deduce the timezone offset. Of course, the author
tried his best to cover all the methods he could foresee, but it's always good
to have another set of eyes try it out.</listitem>
     <listitem>Similarly, is there any way to confuse the <link
linkend="contentpolicy">content policy</link>
mentioned above to cause it to allow certain types of page fetches? For
example, it was recently discovered that favicons are not fetched by the
content, but the chrome itself, hence the content policy did not look up the
correct window to determine the current Tor tag for the favicon fetch. Are
there other things that can do this? Popups? Bookmarklets? Active bookmarks? </listitem>
     <listitem>Alternate ways to store and fetch unique identifiers. For example, <ulink
url="http://developer.mozilla.org/en/docs/DOM:Storage">DOM Storage</ulink>
caught us off guard. 
It was
also discovered by <ulink url="http://pseudo-flaw.net">Gregory
Fleischer</ulink> that <ulink
url="http://pseudo-flaw.net/content/tor/torbutton/">content window access to
chrome</ulink> can be used to build <link linkend="fingerprinting">unique
identifiers</link>.
Are there any other
arcane or experimental ways that Firefox provides to create and store unique
identifiers? Or perhaps unique identifiers can be queried or derived from
properties of the machine/browser that JavaScript has access to? How unique
can these identifiers be?
     </listitem>
     <listitem>Is it possible to get the browser to write some history to disk
(aside from swap) that can be retrieved later? By default, Torbutton should
write no history, cookie, or other browsing activity information to the
harddisk.</listitem>
     <listitem>Do popup windows make it easier to break any of the above
behavior? Are javascript events still canceled in popups? What about recursive
popups from JavaScript, data, and other funky URL types? What about CSS
popups? Are they still blocked after Tor is toggled?</listitem>
     <listitem>Chrome-escalation attacks. The interaction between the
Torbutton chrome JavaScript and the client content window javascript is pretty
well-defined and carefully constructed, but perhaps there is a way to smuggle
javascript back in a return value, or otherwise inject network-loaded
javascript into the chrome (and thus gain complete control of the browser).
</listitem>
</itemizedlist>

    </para>
   </sect3>
  </sect2>
</sect1>
-->
<appendix id="Transparency">
<title>Towards Transparency in Navigation Tracking</title>
<para>

The <link linkend="privacy">privacy properties</link> of Tor Browser are based
upon the assumption that link-click navigation indicates user consent to
tracking between the linking site and the destination site.  While this
definition is sufficient to allow us to eliminate cross-site third party
tracking with only minimal site breakage, it is our long-term goal to further
reduce cross-origin click navigation tracking to mechanisms that are
detectable by attentive users, so they can alert the general public if
cross-origin click navigation tracking is happening where it should not be.

</para>
<para>

In an ideal world, the mechanisms of tracking that can be employed during a
link click would be limited to the contents of URL parameters and other
properties that are fully visible to the user before they click. However, the
entrenched nature of certain archaic web features make it impossible for us to
achieve this transparency goal by ourselves without substantial site breakage.
So, instead we maintain a <link linkend="deprecate">Deprecation
Wishlist</link> of archaic web technologies that are currently being (ab)used
to facilitate federated login and other legitimate click-driven cross-domain
activity but that can one day be replaced with more privacy friendly,
auditable alternatives.

</para>
<para>

Because the total elimination of side channels during cross-origin navigation
will undoubtedly break federated login as well as destroy ad revenue, we
also describe auditable alternatives and promising web draft standards that would
preserve this functionality while still providing transparency when tracking is
occurring.

</para>

<sect1 id="deprecate">
 <title>Deprecation Wishlist</title>
 <orderedlist>
  <listitem><command>The Referer Header</command>
  <para>

When leaving a .onion domain we set the Referer header to an empty string by
<ulink url="https://gitweb.torproject.org/tor-browser.git/commit/?h=tor-browser-52.5.2esr-7.0-2&amp;id=021bffff111b6b93eecb5859e680d540991c20c9">
providing a preference</ulink>, <command>network.http.referer.hideOnionSource</command>, and setting it to <command>true</command>. That avoids leaking
information which might be especially problematic in the case of transitioning
from a .onion domain to one reached over clearnet. Apart from that we haven't
disabled or restricted the Referer ourselves because of the non-trivial number
of sites that rely on the Referer header to "authenticate" image requests and
deep-link navigation on their sites. Furthermore, there seems to be no real
privacy benefit to taking this action by itself in a vacuum, because many sites
have begun encoding Referer URL information into GET parameters when they need
it to cross HTTP to HTTPS scheme transitions. Google's +1 buttons are the best
example of this activity.

  </para>
  <para>

Because of the availability of these other explicit vectors, we believe the
main risk of the Referer header is through inadvertent and/or covert data
leakage. In fact, <ulink
url="http://web2.research.att.com/export/sites/att_labs/people/Krishnamurthy_Balachander/papers/wosn09.pdf">
a great deal of personal data</ulink> is inadvertently leaked to third parties
through the source URL parameters.

  </para>
  <para>

We believe the Referer header should be made explicit, and believe that Referrer
Policy, which is available since Firefox 52, provides a <ulink
url="https://w3c.github.io/webappsec-referrer-policy/#referrer-policy-header">
decent step in this direction</ulink>. If a site wishes to transmit its URL to
third party content elements during load or during link-click, it should have
to specify this as a property of the associated <ulink url="https://blog.mozilla.org/security/2015/01/21/meta-referrer/">
HTML tag</ulink> or in an HTTP response header. With an explicit property or
response header, it would then be possible for the user agent to inform the user
if they are about to click on a link that will transmit Referer information
(perhaps through something as subtle as a different color in the lower toolbar
for the destination URL). This same UI notification can also be used for links
with the <ulink url="https://developers.whatwg.org/links.html#ping">"ping"</ulink>
attribute.

  </para>
  </listitem>
  <listitem><command>window.name</command>
   <para>
<ulink
url="https://developer.mozilla.org/En/DOM/Window.name">window.name</ulink> is
a DOM property that for some reason is allowed to retain a persistent value
for the lifespan of a browser tab. It is possible to utilize this property for
<ulink url="https://www.thomasfrank.se/sessionvars.html">identifier
storage</ulink> during click navigation. This is sometimes used for additional
CSRF protection and federated login.
   </para>
   <para>

It's our opinion that the contents of window.name should not be preserved for
cross-origin navigation, but doing so may break federated login for some sites.

   </para>
  </listitem>
  <listitem><command>JavaScript link rewriting</command>
   <para>

In general, it should not be possible for onclick handlers to alter the
navigation destination of 'a' tags, silently transform them into POST
requests, or otherwise create situations where a user believes they are
clicking on a link leading to one URL that ends up on another. This
functionality is deceptive and is frequently a vector for malware and phishing
attacks. Unfortunately, many legitimate sites also employ such transparent
link rewriting, and blanket disabling this functionality ourselves will simply
cause Tor Browser to fail to navigate properly on these sites.

   </para>
   <para>

Automated cross-origin redirects are one form of this behavior that is
possible for us to <ulink
url="https://trac.torproject.org/projects/tor/ticket/3600">address
ourselves</ulink>, as they are comparatively rare and can be handled with site
permissions.

   </para>
  </listitem>
 </orderedlist>
</sect1>
<sect1>
 <title>Promising Standards</title>
 <orderedlist>
  <listitem><ulink url="https://web.archive.org/web/20130213034335/http://web-send.org:80/">Web-Send Introducer</ulink>
   <para>

Web-Send is a browser-based link sharing and federated login widget that is
designed to operate without relying on third-party tracking or abusing other
cross-origin link-click side channels. It has a compelling list of <ulink
url="https://web.archive.org/web/20130213034335/http://web-send.org:80/featurs.html">
privacy and security features</ulink>, especially if used as a "Like button"
replacement.

   </para>
  </listitem>
  <listitem><ulink url="https://developer.mozilla.org/en-US/docs/Persona">Mozilla Persona</ulink>
   <para>

Mozilla's Persona is designed to provide decentralized, cryptographically
authenticated federated login in a way that does not expose the user to third
party tracking or require browser redirects or side channels. While it does
not directly provide the link sharing capabilities that Web-Send does, it is a
better solution to the privacy issues associated with federated login than
Web-Send is.

   </para>
  </listitem>
 </orderedlist>
</sect1>
</appendix>
</article>
